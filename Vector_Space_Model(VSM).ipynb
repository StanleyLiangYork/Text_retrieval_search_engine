{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vector Space Model(VSM).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanleyLiangYork/Text_retrieval_search_engine/blob/main/Vector_Space_Model(VSM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXuVi0vqZjBu"
      },
      "source": [
        "\n",
        "\n",
        "# Vector Space Model\n",
        "\n",
        "This tutorial is a walk-through for the implementation of the vector space model (VSM) for text retrieval.\n",
        "* You should copy the dataset (zip file) containing 22,000 text files to your  VM.\n",
        "* You can mount your google drive on the VM and copy the dataset to your own google drive.\n",
        "* You will implement the VSM by following the step in this notebook.\n",
        "* After the VSM is set, you can give a query and the model should return the top five documents related to the query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChutGUrV8qQm",
        "outputId": "1880870d-5f93-49ab-af19-feeabf71c88a"
      },
      "source": [
        "# Copy the dataset to the VM file system.\n",
        "!wget https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip documents.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-28 01:35:24--  https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.215.128, 173.194.216.128, 173.194.217.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.215.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226023 (221K) [application/x-zip-compressed]\n",
            "Saving to: ‘documents.zip’\n",
            "\n",
            "\rdocuments.zip         0%[                    ]       0  --.-KB/s               \rdocuments.zip       100%[===================>] 220.73K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-10-28 01:35:24 (121 MB/s) - ‘documents.zip’ saved [226023/226023]\n",
            "\n",
            "--2021-10-28 01:35:24--  http://documents.zip/\n",
            "Resolving documents.zip (documents.zip)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘documents.zip’\n",
            "FINISHED --2021-10-28 01:35:24--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 221K in 0.002s (121 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqzAPOgO9dQv"
      },
      "source": [
        "Unzip the text document dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZzylvqDzBC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2368b5-3f3a-471c-d915-065c83710733"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = '/content/documents.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r',) as zip:\n",
        "  zip.extractall()\n",
        "  print('Done!!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olrQF1eM1Ts8"
      },
      "source": [
        "# uncomment this code if you want to mount your google drive to the VM\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv4ngG72aHKK"
      },
      "source": [
        "**Bellow cell imports all the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkFO0aH-8W5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7958878-11ff-44d2-a3a9-ad9f518e4398"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('popular');\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJd9bSh7aRn5"
      },
      "source": [
        "# Parse the text document dataset into a dictionary\n",
        "\n",
        "The get_docDict() function takes the dataset folder path\n",
        "\n",
        "removes the extra \"\\n\" end of line symbols\n",
        "\n",
        "returns a dictionary structure {filename : text content}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeT-JtmRlz7h"
      },
      "source": [
        "def get_docDict(path):\n",
        "  doc_dict = {}\n",
        "  file_names = os.listdir(path)\n",
        "\n",
        "  for file in file_names:\n",
        "    full_path = path+'/'+file\n",
        "    with open(full_path, 'r', errors='ignore') as f:\n",
        "      data = f.readlines()\n",
        "    text = \"\".join([i for i in data])\n",
        "    # remove all the \"\\n\" from the text\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    doc_dict[file] = text\n",
        "  return doc_dict"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vI-BIyJRq5A",
        "outputId": "3bcaefa3-20b3-4a2f-8c9d-31203e89eabc"
      },
      "source": [
        "path = '/content/documents'\n",
        "\n",
        "doc_dict = get_docDict(path)\n",
        "doc_dict.keys()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['A00-1012.pdf.txt', 'A00-1005.pdf.txt', 'A00-1000.pdf.txt', 'A00-1015.pdf.txt', 'A00-1006.pdf.txt', 'A00-1019.pdf.txt', 'A00-1018.pdf.txt', 'A00-1009.pdf.txt', 'A00-1016.pdf.txt', 'A00-1004.pdf.txt', 'A00-1011.pdf.txt', 'A00-1002.pdf.txt', 'A00-1003.pdf.txt', 'A00-1010.pdf.txt', 'A00-1013.pdf.txt', 'A00-1007.pdf.txt', 'A00-1001.pdf.txt', 'A00-1008.pdf.txt', 'A00-1020.pdf.txt', 'A00-1017.pdf.txt', 'A00-1014.pdf.txt'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2f2SMGJaVnO"
      },
      "source": [
        "# Clean the text\n",
        "The clean_text() function perform the following tasks:\n",
        "* remove extra white space\n",
        "* remove extra dots \"...\" between lines in original document text\n",
        "* remove extra hyphen\n",
        "* tokenize: text string ==> a list of tokens\n",
        "* remove stop words and punctuation (English)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsYLF3lVk2iK"
      },
      "source": [
        "def clean_text(doc_dict):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : text}\n",
        "  output - a dictionary of {filename : clean text} \n",
        "\n",
        "  \"\"\"\n",
        "  clean_dict = {}\n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  \n",
        "  for name, doc in doc_dict.items():\n",
        "    # remove extra white space\n",
        "    text = re.sub(r\"\\s+\", \" \", doc)\n",
        "    # remove extra ...\n",
        "    text = re.sub(r\"\\.+\",\" \", doc)\n",
        "    # remove hyphen\n",
        "    text = re.sub(r\"-\",\"\", text)\n",
        "    text = text.lower()\n",
        "    text_tokens = word_tokenize(text)\n",
        "    text_clean = []\n",
        "    for word in text_tokens:\n",
        "      if (word not in stopwords_english and word not in string.punctuation):\n",
        "        # stem_word = stemmer.stem(word)\n",
        "        text_clean.append(word)\n",
        "    \n",
        "    clean_dict[name] = text_clean\n",
        "    \n",
        "  return clean_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeB7zv9uh_em"
      },
      "source": [
        "clean_dict = clean_text(doc_dict)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJRTz1dpkQZt",
        "outputId": "5e33cf84-f531-49d3-c21b-c6688c9165f2"
      },
      "source": [
        "clean_dict['A00-1000.pdf.txt']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['association',\n",
              " 'computational',\n",
              " 'linguistics',\n",
              " '6',\n",
              " 'th',\n",
              " 'applied',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'conference',\n",
              " 'proceedings',\n",
              " 'conference',\n",
              " 'april',\n",
              " '29may',\n",
              " '4',\n",
              " '2000',\n",
              " 'seattle',\n",
              " 'washington',\n",
              " 'usa',\n",
              " 'anlp',\n",
              " '2000preface',\n",
              " '131',\n",
              " 'papers',\n",
              " 'submitted',\n",
              " 'anlp2000',\n",
              " '46',\n",
              " 'accepted',\n",
              " 'presentation',\n",
              " 'conference',\n",
              " 'papers',\n",
              " 'came',\n",
              " '24',\n",
              " 'countries',\n",
              " 'fifty',\n",
              " 'eight',\n",
              " 'united',\n",
              " 'states',\n",
              " 'america',\n",
              " 'eleven',\n",
              " 'germany',\n",
              " 'united',\n",
              " 'kingdom',\n",
              " 'nine',\n",
              " 'canada',\n",
              " 'eight',\n",
              " 'japan',\n",
              " 'four',\n",
              " 'italy',\n",
              " 'spain',\n",
              " 'three',\n",
              " 'ach',\n",
              " 'france',\n",
              " 'korea',\n",
              " 'switzerland',\n",
              " 'two',\n",
              " 'australia',\n",
              " 'china',\n",
              " 'netherlands',\n",
              " 'sweden',\n",
              " 'one',\n",
              " 'czech',\n",
              " 'republic',\n",
              " 'denmark',\n",
              " 'finland',\n",
              " 'greece',\n",
              " 'india',\n",
              " 'hong',\n",
              " 'kong',\n",
              " 'malaysia',\n",
              " 'norway',\n",
              " 'russia',\n",
              " 'taiwan',\n",
              " '40',\n",
              " 'papers',\n",
              " 'submitted',\n",
              " 'industry',\n",
              " '85',\n",
              " 'papers',\n",
              " 'came',\n",
              " 'academia',\n",
              " '2',\n",
              " 'papers',\n",
              " 'submitted',\n",
              " 'government',\n",
              " 'organizations',\n",
              " 'four',\n",
              " 'submissions',\n",
              " 'combined',\n",
              " 'reviewing',\n",
              " 'process',\n",
              " 'supported',\n",
              " 'webbased',\n",
              " 'reviewer',\n",
              " 'interface',\n",
              " 'developed',\n",
              " 'elisha',\n",
              " 'kane',\n",
              " 'new',\n",
              " 'mexico',\n",
              " 'state',\n",
              " 'university',\n",
              " \"'s\",\n",
              " 'computing',\n",
              " 'research',\n",
              " 'lab',\n",
              " 'linda',\n",
              " 'fresques',\n",
              " 'crl',\n",
              " 'coordinated',\n",
              " 'refereeing',\n",
              " 'process',\n",
              " 'would',\n",
              " 'like',\n",
              " 'express',\n",
              " 'gratitude',\n",
              " 'appreciation',\n",
              " 'fthe',\n",
              " 'program',\n",
              " 'committee',\n",
              " 'members',\n",
              " 'responsible',\n",
              " 'six',\n",
              " 'areas',\n",
              " 'lynn',\n",
              " 'carlson',\n",
              " 'tools',\n",
              " 'resources',\n",
              " 'developing',\n",
              " 'nlp',\n",
              " 'systems',\n",
              " 'subcommittee',\n",
              " 'eduard',\n",
              " 'hovy',\n",
              " 'integrated',\n",
              " 'nlp',\n",
              " 'systems',\n",
              " 'subcommittee',\n",
              " 'richard',\n",
              " 'kittredge',\n",
              " 'multilingual',\n",
              " 'text',\n",
              " 'processing',\n",
              " 'subcommittee',\n",
              " 'ray',\n",
              " 'perrault',\n",
              " 'spoken',\n",
              " 'language',\n",
              " 'systems',\n",
              " 'subcommittee',\n",
              " 'oliviero',\n",
              " 'stock',\n",
              " 'monolingual',\n",
              " 'text',\n",
              " 'processing',\n",
              " 'systems',\n",
              " 'subcommittee',\n",
              " 'john',\n",
              " 'white',\n",
              " 'evaluation',\n",
              " 'subcommittee',\n",
              " 'following',\n",
              " 'colleagues',\n",
              " 'doug',\n",
              " 'appelt',\n",
              " 'fabio',\n",
              " 'ciravegna',\n",
              " 'robert',\n",
              " 'dale',\n",
              " 'michael',\n",
              " 'elhadad',\n",
              " 'ralph',\n",
              " 'grishman',\n",
              " 'lynette',\n",
              " 'hirschman',\n",
              " 'yuval',\n",
              " 'krymolowski',\n",
              " 'inderjeet',\n",
              " 'mani',\n",
              " 'zvi',\n",
              " 'marx',\n",
              " 'martha',\n",
              " 'palmer',\n",
              " 'harold',\n",
              " 'somers',\n",
              " 'toshiyuki',\n",
              " 'takezawa',\n",
              " 'takehito',\n",
              " 'utsuro',\n",
              " 'dekai',\n",
              " 'wu',\n",
              " 'bulk',\n",
              " 'reviewing',\n",
              " 'igor',\n",
              " 'boguslavsky',\n",
              " 'jim',\n",
              " 'cowie',\n",
              " 'john',\n",
              " 'dowding',\n",
              " 'jim',\n",
              " 'glass',\n",
              " 'jan',\n",
              " 'haji~',\n",
              " 'pierre',\n",
              " 'isabeiie',\n",
              " 'alberto',\n",
              " 'lavelli',\n",
              " 'daniel',\n",
              " 'marcu',\n",
              " 'david',\n",
              " 'mcdonald',\n",
              " 'owen',\n",
              " 'rainbow',\n",
              " 'tomek',\n",
              " 'strzalkowski',\n",
              " 'kathryn',\n",
              " 'b',\n",
              " 'taylor',\n",
              " 'pick',\n",
              " 'vossen',\n",
              " 'r6mi',\n",
              " 'zajac',\n",
              " 'david',\n",
              " 'carter',\n",
              " 'ido',\n",
              " 'dagan',\n",
              " 'andreas',\n",
              " 'eiscle',\n",
              " 'oren',\n",
              " 'glikman',\n",
              " 'donna',\n",
              " 'harman',\n",
              " 'tanya',\n",
              " 'korelsky',\n",
              " 'chinyew',\n",
              " 'lin',\n",
              " 'paul',\n",
              " 'martin',\n",
              " 'teruko',\n",
              " 'mitamura',\n",
              " 'norbert',\n",
              " 'reithinger',\n",
              " 'beth',\n",
              " 'sundheim',\n",
              " 'hans',\n",
              " 'uszkoreit',\n",
              " 'ralph',\n",
              " 'weischedel',\n",
              " 'believe',\n",
              " 'quality',\n",
              " 'papers',\n",
              " 'elected',\n",
              " 'israther',\n",
              " 'high',\n",
              " 'hope',\n",
              " 'conference',\n",
              " 'success',\n",
              " 'sergei',\n",
              " 'nirenburg',\n",
              " 'chair',\n",
              " 'program',\n",
              " 'committee',\n",
              " 'anlp2000',\n",
              " 'anlpi',\n",
              " 'anlp',\n",
              " 'table',\n",
              " 'contents',\n",
              " 'section',\n",
              " '1',\n",
              " 'applied',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'conference',\n",
              " 'anlp',\n",
              " 'anlp',\n",
              " 'preface',\n",
              " 'list',\n",
              " 'reviewers',\n",
              " 'sergei',\n",
              " 'nirenburg',\n",
              " 'program',\n",
              " 'committee',\n",
              " 'chair',\n",
              " 'anlpi',\n",
              " 'bustuca',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'bus',\n",
              " 'route',\n",
              " 'oracle',\n",
              " 'tore',\n",
              " 'amble',\n",
              " '1',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'close',\n",
              " 'languages',\n",
              " 'jan',\n",
              " 'haji~',\n",
              " 'jan',\n",
              " 'hric',\n",
              " 'vladislav',\n",
              " 'kubofi',\n",
              " '7',\n",
              " 'crosslanguage',\n",
              " 'multimedia',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " 'sharon',\n",
              " 'flank',\n",
              " '13',\n",
              " 'automatic',\n",
              " 'construction',\n",
              " 'parallel',\n",
              " 'englishchinese',\n",
              " 'corpus',\n",
              " 'crosslanguage',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " 'jiang',\n",
              " 'chen',\n",
              " 'jianyun',\n",
              " 'nie',\n",
              " '21',\n",
              " 'partsld',\n",
              " 'dialoguebased',\n",
              " 'system',\n",
              " 'identifying',\n",
              " 'parts',\n",
              " 'medical',\n",
              " 'systems',\n",
              " 'amit',\n",
              " 'bagga',\n",
              " 'tomek',\n",
              " 'strzalkowski',\n",
              " 'g',\n",
              " 'bowden',\n",
              " 'wise',\n",
              " '29',\n",
              " 'translation',\n",
              " 'using',\n",
              " 'information',\n",
              " 'dialogue',\n",
              " 'participants',\n",
              " 'setsuo',\n",
              " 'yamada',\n",
              " 'eiichiro',\n",
              " 'sumita',\n",
              " 'hideki',\n",
              " 'kashioka',\n",
              " '37',\n",
              " 'distilling',\n",
              " 'dialoguesa',\n",
              " 'method',\n",
              " 'using',\n",
              " 'natural',\n",
              " 'dialogue',\n",
              " 'corpora',\n",
              " 'dialogue',\n",
              " 'systems',\n",
              " 'development',\n",
              " 'arne',\n",
              " 'j6nsson',\n",
              " 'nils',\n",
              " 'dahlb~ick',\n",
              " '44',\n",
              " 'planbased',\n",
              " 'dialogue',\n",
              " 'management',\n",
              " 'ina',\n",
              " 'physics',\n",
              " 'tour',\n",
              " 'reva',\n",
              " 'freedman',\n",
              " '52',\n",
              " 'framework',\n",
              " 'mt',\n",
              " 'multilingual',\n",
              " 'nlg',\n",
              " 'systems',\n",
              " 'based',\n",
              " 'uniform',\n",
              " 'lexicostructural',\n",
              " 'processing',\n",
              " 'benoit',\n",
              " 'lavoie',\n",
              " 'richard',\n",
              " 'kittredge',\n",
              " 'tanya',\n",
              " 'korelsky',\n",
              " 'owen',\n",
              " 'rambow',\n",
              " '60',\n",
              " \"talk'n'travel\",\n",
              " 'conversational',\n",
              " 'system',\n",
              " 'air',\n",
              " 'travel',\n",
              " 'planning',\n",
              " 'david',\n",
              " 'stallard',\n",
              " '68',\n",
              " 'rees',\n",
              " 'largescale',\n",
              " 'relation',\n",
              " 'event',\n",
              " 'extraction',\n",
              " 'system',\n",
              " 'chinatsu',\n",
              " 'aone',\n",
              " 'mila',\n",
              " 'ramossantacruz',\n",
              " '76',\n",
              " 'experiments',\n",
              " 'sentence',\n",
              " 'boundary',\n",
              " 'detection',\n",
              " 'mark',\n",
              " 'stevenson',\n",
              " 'robert',\n",
              " 'gaizauskas',\n",
              " '84',\n",
              " 'dp',\n",
              " 'detector',\n",
              " 'presuppositions',\n",
              " 'survey',\n",
              " 'questions',\n",
              " 'katja',\n",
              " 'wiemerhastings',\n",
              " 'peter',\n",
              " 'wiemerhastings',\n",
              " 'sonya',\n",
              " 'rajan',\n",
              " 'art',\n",
              " 'graesser',\n",
              " 'roger',\n",
              " 'kreuz',\n",
              " 'ashish',\n",
              " 'karnavat',\n",
              " '90',\n",
              " 'anlpiii',\n",
              " 'mimic',\n",
              " 'adaptive',\n",
              " 'mixed',\n",
              " 'initiative',\n",
              " 'spoken',\n",
              " 'dialogue',\n",
              " 'system',\n",
              " 'information',\n",
              " 'queries',\n",
              " 'jennifer',\n",
              " 'chucarroll',\n",
              " '97',\n",
              " 'javox',\n",
              " 'toolkit',\n",
              " 'building',\n",
              " 'speechenabled',\n",
              " 'applications',\n",
              " 'michael',\n",
              " 'fulkerson',\n",
              " 'alan',\n",
              " 'w',\n",
              " 'biermann',\n",
              " '105',\n",
              " 'compact',\n",
              " 'architecture',\n",
              " 'dialogue',\n",
              " 'management',\n",
              " 'based',\n",
              " 'scripts',\n",
              " 'metaoutputs',\n",
              " 'manny',\n",
              " 'rayner',\n",
              " 'beth',\n",
              " 'ann',\n",
              " 'hockey',\n",
              " 'frankie',\n",
              " 'james',\n",
              " '112',\n",
              " 'representation',\n",
              " 'complex',\n",
              " 'evolving',\n",
              " 'data',\n",
              " 'dependencies',\n",
              " 'generation',\n",
              " 'c',\n",
              " 'mellish',\n",
              " 'r',\n",
              " 'evans',\n",
              " 'l',\n",
              " 'cahill',\n",
              " 'c',\n",
              " 'doran',\n",
              " 'paiva',\n",
              " 'reape',\n",
              " 'scott',\n",
              " 'n',\n",
              " 'tipper',\n",
              " '119',\n",
              " 'automatic',\n",
              " 'reviser',\n",
              " 'transcheck',\n",
              " 'system',\n",
              " 'jeanmarc',\n",
              " 'jutras',\n",
              " '127',\n",
              " 'unit',\n",
              " 'completion',\n",
              " 'computeraided',\n",
              " 'translation',\n",
              " 'typing',\n",
              " 'system',\n",
              " 'philippe',\n",
              " 'langlais',\n",
              " 'george',\n",
              " 'foster',\n",
              " 'guy',\n",
              " 'lapalme',\n",
              " '135',\n",
              " 'multilingual',\n",
              " 'coreference',\n",
              " 'resolution',\n",
              " 'sanda',\n",
              " 'harabagiu',\n",
              " 'steven',\n",
              " 'j',\n",
              " 'maiorano',\n",
              " '142',\n",
              " 'ranking',\n",
              " 'suspected',\n",
              " 'answers',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'questions',\n",
              " 'using',\n",
              " 'predictive',\n",
              " 'annotation',\n",
              " 'dragomir',\n",
              " 'r',\n",
              " 'radev',\n",
              " 'john',\n",
              " 'prager',\n",
              " 'valerie',\n",
              " 'samn',\n",
              " '150',\n",
              " 'message',\n",
              " 'classification',\n",
              " 'call',\n",
              " 'center',\n",
              " 'stephan',\n",
              " 'busemann',\n",
              " 'sven',\n",
              " 'schmeier',\n",
              " 'roman',\n",
              " 'g',\n",
              " 'arens',\n",
              " '158',\n",
              " 'question',\n",
              " 'answering',\n",
              " 'system',\n",
              " 'supported',\n",
              " 'information',\n",
              " 'extraction',\n",
              " 'rohini',\n",
              " 'srihari',\n",
              " 'wei',\n",
              " 'li',\n",
              " '166',\n",
              " 'categorizing',\n",
              " 'unknown',\n",
              " 'words',\n",
              " 'using',\n",
              " 'decision',\n",
              " 'trees',\n",
              " 'identify',\n",
              " 'names',\n",
              " 'misspellings',\n",
              " 'janine',\n",
              " 'toole',\n",
              " '173',\n",
              " 'examining',\n",
              " 'role',\n",
              " 'statistical',\n",
              " 'linguistic',\n",
              " 'knowledge',\n",
              " 'sources',\n",
              " 'generalknowledge',\n",
              " 'questionanswering',\n",
              " 'system',\n",
              " 'claire',\n",
              " 'cardie',\n",
              " 'vincent',\n",
              " 'ng',\n",
              " 'david',\n",
              " 'pierce',\n",
              " 'chris',\n",
              " 'buckley',\n",
              " '180',\n",
              " 'extracting',\n",
              " 'molecular',\n",
              " 'binding',\n",
              " 'relationships',\n",
              " 'biomedical',\n",
              " 'text',\n",
              " 'thomas',\n",
              " 'c',\n",
              " 'rindflesch',\n",
              " 'jayant',\n",
              " 'v',\n",
              " 'rajan',\n",
              " 'lawrence',\n",
              " 'hunter',\n",
              " '188',\n",
              " 'compound',\n",
              " 'noun',\n",
              " 'segmentation',\n",
              " 'based',\n",
              " 'lexical',\n",
              " 'data',\n",
              " 'extracted',\n",
              " 'corpus',\n",
              " 'juntae',\n",
              " 'yoon',\n",
              " '196',\n",
              " 'experiments',\n",
              " 'corpusbased',\n",
              " 'lfg',\n",
              " 'specialization',\n",
              " 'nicola',\n",
              " 'cancedda',\n",
              " 'christer',\n",
              " 'samuelsson',\n",
              " '204',\n",
              " 'tool',\n",
              " 'automated',\n",
              " 'revision',\n",
              " 'grammars',\n",
              " 'nlp',\n",
              " 'systems',\n",
              " 'nanda',\n",
              " 'kambhatla',\n",
              " 'wlodek',\n",
              " 'zadrozny',\n",
              " '210',\n",
              " 'anlpiv',\n",
              " 'aggressive',\n",
              " 'morphology',\n",
              " 'robust',\n",
              " 'lexical',\n",
              " 'coverage',\n",
              " 'william',\n",
              " 'woods',\n",
              " '218',\n",
              " 'tnta',\n",
              " 'statistical',\n",
              " 'partofspeech',\n",
              " 'tagger',\n",
              " 'thorsten',\n",
              " 'brants',\n",
              " '224',\n",
              " 'language',\n",
              " 'independent',\n",
              " 'morphological',\n",
              " 'analysis',\n",
              " 'tatsuo',\n",
              " 'yamashita',\n",
              " 'yuji',\n",
              " 'matsumoto',\n",
              " '232',\n",
              " 'divideandconquer',\n",
              " 'strategy',\n",
              " 'shallow',\n",
              " 'parsing',\n",
              " 'german',\n",
              " 'free',\n",
              " 'texts',\n",
              " 'giinter',\n",
              " 'neumann',\n",
              " 'christian',\n",
              " 'braun',\n",
              " 'jakub',\n",
              " 'piskorski',\n",
              " '239',\n",
              " 'hybrid',\n",
              " 'approach',\n",
              " 'named',\n",
              " 'entity',\n",
              " 'subtype',\n",
              " 'tagging',\n",
              " 'rohini',\n",
              " 'srihari',\n",
              " 'cheng',\n",
              " 'niu',\n",
              " 'wei',\n",
              " 'li',\n",
              " '247',\n",
              " 'spelling',\n",
              " 'grammar',\n",
              " 'correction',\n",
              " 'danish',\n",
              " 'scarrie',\n",
              " 'patrizia',\n",
              " 'paggio',\n",
              " '255',\n",
              " 'linguistic',\n",
              " 'knowledge',\n",
              " 'improve',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " 'william',\n",
              " 'woods',\n",
              " 'lawrence',\n",
              " 'bookman',\n",
              " 'ann',\n",
              " 'houston',\n",
              " 'robert',\n",
              " 'j',\n",
              " 'kuhns',\n",
              " 'paul',\n",
              " 'martin',\n",
              " 'stephen',\n",
              " 'green',\n",
              " '262',\n",
              " 'domainspecific',\n",
              " 'knowledge',\n",
              " 'acquisition',\n",
              " 'text',\n",
              " 'dan',\n",
              " 'moldovan',\n",
              " 'roxana',\n",
              " 'girju',\n",
              " 'vasile',\n",
              " 'rus',\n",
              " '268',\n",
              " 'largescale',\n",
              " 'controlled',\n",
              " 'vocabulary',\n",
              " 'indexing',\n",
              " 'named',\n",
              " 'entities',\n",
              " 'mark',\n",
              " 'wasson',\n",
              " '276',\n",
              " 'unsupervised',\n",
              " 'discovery',\n",
              " 'scenariolevel',\n",
              " 'patterns',\n",
              " 'information',\n",
              " 'extraction',\n",
              " 'roman',\n",
              " 'yangarber',\n",
              " 'ralph',\n",
              " 'grishman',\n",
              " 'pasi',\n",
              " 'tapanainen',\n",
              " 'silja',\n",
              " 'huttunen',\n",
              " '282',\n",
              " 'using',\n",
              " 'corpusderived',\n",
              " 'name',\n",
              " 'lists',\n",
              " 'named',\n",
              " 'entity',\n",
              " 'recognition',\n",
              " 'mark',\n",
              " 'stevenson',\n",
              " 'robert',\n",
              " 'gaizauskas',\n",
              " '290',\n",
              " 'answer',\n",
              " 'extraction',\n",
              " 'steven',\n",
              " 'abney',\n",
              " 'michael',\n",
              " 'collins',\n",
              " 'amit',\n",
              " 'singhal',\n",
              " '296',\n",
              " 'evaluation',\n",
              " 'automatically',\n",
              " 'identified',\n",
              " 'index',\n",
              " 'terms',\n",
              " 'browsing',\n",
              " 'electronic',\n",
              " 'documents',\n",
              " 'nina',\n",
              " 'wacholder',\n",
              " 'judith',\n",
              " 'l',\n",
              " 'klavans',\n",
              " 'david',\n",
              " 'k',\n",
              " 'evans',\n",
              " '302',\n",
              " 'sentence',\n",
              " 'reduction',\n",
              " 'automatic',\n",
              " 'text',\n",
              " 'summarization',\n",
              " 'hongyan',\n",
              " 'jing',\n",
              " '310',\n",
              " 'named',\n",
              " 'entity',\n",
              " 'extraction',\n",
              " 'noisy',\n",
              " 'input',\n",
              " 'speech',\n",
              " 'ocr',\n",
              " 'david',\n",
              " 'miller',\n",
              " 'sean',\n",
              " 'boisen',\n",
              " 'richard',\n",
              " 'schwartz',\n",
              " 'rebecca',\n",
              " 'stone',\n",
              " 'ralph',\n",
              " 'weischedel',\n",
              " '316',\n",
              " 'improving',\n",
              " 'testsuites',\n",
              " 'via',\n",
              " 'instrumentation',\n",
              " 'norbert',\n",
              " 'br6ker',\n",
              " '325',\n",
              " 'efficiency',\n",
              " 'multimodal',\n",
              " 'interaction',\n",
              " 'mapbased',\n",
              " 'task',\n",
              " 'philip',\n",
              " 'cohen',\n",
              " 'david',\n",
              " 'mcgee',\n",
              " 'josh',\n",
              " 'clow',\n",
              " '331',\n",
              " 'anlpv']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4qq75WF3HOs"
      },
      "source": [
        "# Make the vocabulary of whole document dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WDu9CjM3FQW"
      },
      "source": [
        "def make_vocab(doc_dict):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text} \n",
        "  output - a set of unique terms forms the dataset vocabulary\n",
        "  \"\"\"\n",
        "  total_tokens = []\n",
        "  for tokens in doc_dict.values():\n",
        "    total_tokens += tokens\n",
        "  vocab = list(set(total_tokens))\n",
        "  return vocab"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7e0jm656vYw",
        "outputId": "a4b004eb-383f-4f9f-eee9-75d12c7ddfed"
      },
      "source": [
        "vocab = make_vocab(clean_dict)\n",
        "len(vocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10811"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDqAbaxRz0zu"
      },
      "source": [
        "# Calculate term frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkjTguyBzuhU"
      },
      "source": [
        "def get_DocTF(doc_dict, vocab):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text}, the vocabulary of the whole dataset\n",
        "  output - a dictionary of {filename : {term : count}}\n",
        "  \"\"\"\n",
        "  tf_dict = {}\n",
        "  # make the dict for filename=>{term:frequency}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_dict[doc_id] = {}\n",
        "\n",
        "  for word in vocab:\n",
        "    for doc_id, text in doc_dict.items():\n",
        "      tf_dict[doc_id][word] = text.count(word)\n",
        "    \n",
        "  return tf_dict"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyB6P_Ou2Dht"
      },
      "source": [
        "tf_dict = get_DocTF(clean_dict, vocab)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6gSs515_iAp",
        "outputId": "2d5b4433-918e-4ae5-9e18-a4d7dafa920d"
      },
      "source": [
        "tf_dict['A00-1000.pdf.txt']['language']"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJ9xPqG_w0M"
      },
      "source": [
        "# Calculate document frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZqY34xJ1DF-"
      },
      "source": [
        "def get_DocDF(clean_dict, vocab):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text}, the vocabulary of the whole dataset\n",
        "  output - a dictionary of all terms in the vocabulary - {term : count}\n",
        "  \"\"\"\n",
        "  df_dict = {}\n",
        "  for word in vocab:\n",
        "    freq = 0\n",
        "    for text_tokens in clean_dict.values():\n",
        "      if word in text_tokens:\n",
        "        freq += 1\n",
        "    df_dict[word] = freq\n",
        "\n",
        "  return df_dict"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppSNsZD_zuoD"
      },
      "source": [
        "df_dict = get_DocDF(clean_dict, vocab)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8C2tLSYBTtz",
        "outputId": "e4f160de-0cef-4953-a965-28199b414895"
      },
      "source": [
        "df_dict['language']"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYU55xW2Bvpz"
      },
      "source": [
        "# Calculate inverse document frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxmwGJ8V9Wco"
      },
      "source": [
        "def inverse_DF(df_dict, vocab, doc_length):\n",
        "  \"\"\"\n",
        "  input - a dictionary of DF {term : count}, the vocabulary of the whole dataset, total # of documents in the dataset\n",
        "  output - a dictionary of IDF of all terms in the vocabulary - {term : inver_df}\n",
        "  \"\"\"\n",
        "  idf_dict = {}\n",
        "  for word in vocab:\n",
        "    # idf_dict[word] = - np.log2((df_dict[word]) / (doc_length)) \n",
        "    idf_dict[word] = round(np.log(((doc_length - df_dict[word]+0.5) / (df_dict[word]+0.5))+1), 4)\n",
        "    \n",
        "  return idf_dict\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R01oM-2uSA15"
      },
      "source": [
        "doc_length = len(tf_dict.keys())\n",
        "idf_dict = inverse_DF(df_dict, vocab, doc_length)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK9hs9-9j9XC",
        "outputId": "a54fe04d-6287-4882-f496-b3e6c0dbdffb"
      },
      "source": [
        "idf_dict['text']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1733"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9CTTmCUHPXs"
      },
      "source": [
        "# Calculate TF-IDF\n",
        "\n",
        "A term t in a given document d, TF-IDF(t,d) = TF(t,d) * IDF(t)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxWEl-dHHTpc"
      },
      "source": [
        "def get_tf_idf(tf_dict, idf_dict, doc_dict, vocab):\n",
        "  tf_idf_dict = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_idf_dict[doc_id] = {}\n",
        "  \n",
        "  for word in vocab:\n",
        "    for doc_id, text_tokens in doc_dict.items():\n",
        "      tf_idf_dict[doc_id][word] = round((tf_dict[doc_id][word] * idf_dict[word]), 4)\n",
        "  return tf_idf_dict"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX-8wAh9HS80"
      },
      "source": [
        "tf_idf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dm1wK8YJoP9",
        "outputId": "4b34966a-1978-4ea4-99e7-f03e43b9c283"
      },
      "source": [
        "tf_idf_dict['A00-1001.pdf.txt']['text']"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3466"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdHpP87kTHgb"
      },
      "source": [
        "# Define the Vector Space Model (VSM)\n",
        "\n",
        "To find the relevant documents related to query, pass the query to function along with collection of documents (dictionary) and tf-idf scores (dictionary returned by tfidf). Function returns the top 5 documents from a collection of all documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlAP7DDhkX_K"
      },
      "source": [
        "def vectorSpaceModel(query, doc_dict,tfidf_dict):\n",
        "  query_vocab = []\n",
        "  query = query.lower()\n",
        "  query = re.sub(r\"\\s+\", \" \", query)\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  for word in query.split():\n",
        "    if (word not in string.punctuation and word not in stopwords_english):\n",
        "        query_vocab.append(word)\n",
        "\n",
        "  query_wc = {}\n",
        "  for word in query_vocab:\n",
        "    query_wc[word] = query.split().count(word)\n",
        "\n",
        "  relevance_scores = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    score = 0\n",
        "    for word in query_vocab:\n",
        "      score += query_wc[word] * tf_idf_dict[doc_id][word]\n",
        "    relevance_scores[doc_id] = round(score,4)\n",
        "\n",
        "  # sort the relevance score and get the top-k ranking\n",
        "  # sort the keys of the relevance score by value\n",
        "  sort_keys = sorted(relevance_scores, key=relevance_scores.get , reverse = True)\n",
        "  top_keys = sort_keys[:5]\n",
        "  top_5 = {}\n",
        "  for key in top_keys:\n",
        "    top_5[key] = relevance_scores[key]\n",
        "\n",
        "  return top_5\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcVbE98SAECH"
      },
      "source": [
        "# Test the VSM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s0A16tgAM5m",
        "outputId": "a5fa4d5c-ac23-4445-8663-5a305e57c748"
      },
      "source": [
        "# get the text documents\n",
        "path = '/content/documents'\n",
        "doc_dict = get_docDict(path)\n",
        "\n",
        "# clean the text\n",
        "clean_dict = clean_text(doc_dict)\n",
        "\n",
        "# get the vocabulary of the whole dataset\n",
        "vocab = make_vocab(clean_dict)\n",
        "\n",
        "# get the term frequency (TF)\n",
        "tf_dict = get_DocTF(clean_dict, vocab)\n",
        "\n",
        "# get the document frequency (DF)\n",
        "df_dict = get_DocDF(clean_dict, vocab)\n",
        "\n",
        "# get the inverse document frequency (IDF)\n",
        "doc_length = len(tf_dict.keys())\n",
        "idf_dict = inverse_DF(df_dict, vocab, doc_length)\n",
        "\n",
        "# calculate TF-IDF\n",
        "tf_idf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab)\n",
        "\n",
        "query1 = \"Natural Language\"\n",
        "result1 = vectorSpaceModel(query1, doc_dict,tf_idf_dict)\n",
        "print(result1)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A00-1001.pdf.txt': 2.294, 'A00-1007.pdf.txt': 1.849, 'A00-1005.pdf.txt': 1.2414, 'A00-1016.pdf.txt': 0.951, 'A00-1009.pdf.txt': 0.9114}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stLKl0z44jRS",
        "outputId": "2fa5281d-fe6d-4186-8f20-d0f2808d0989"
      },
      "source": [
        "query2 = \"Data mining\"\n",
        "result2 = vectorSpaceModel(query2, doc_dict,tf_idf_dict)\n",
        "\n",
        "query3 = \"I like text retrieval\"\n",
        "result3 = vectorSpaceModel(query3, doc_dict,tf_idf_dict)\n",
        "\n",
        "query4 = \"probability model and language model\"\n",
        "result4 = vectorSpaceModel(query4, doc_dict,tf_idf_dict)\n",
        "\n",
        "print(result2)\n",
        "print()\n",
        "print(result3)\n",
        "print()\n",
        "print(result4)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A00-1004.pdf.txt': 21.748, 'A00-1020.pdf.txt': 6.334, 'A00-1017.pdf.txt': 5.199, 'A00-1005.pdf.txt': 1.5597, 'A00-1009.pdf.txt': 0.8665}\n",
            "\n",
            "{'A00-1003.pdf.txt': 60.3656, 'A00-1012.pdf.txt': 12.483, 'A00-1004.pdf.txt': 10.4346, 'A00-1018.pdf.txt': 7.1472, 'A00-1020.pdf.txt': 4.8524}\n",
            "\n",
            "{'A00-1004.pdf.txt': 90.612, 'A00-1019.pdf.txt': 63.0706, 'A00-1007.pdf.txt': 13.7778, 'A00-1012.pdf.txt': 12.0552, 'A00-1017.pdf.txt': 11.8802}\n"
          ]
        }
      ]
    }
  ]
}