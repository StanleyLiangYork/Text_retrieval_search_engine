{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vector Space Model(VSM).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXuVi0vqZjBu"
      },
      "source": [
        "\n",
        "\n",
        "# Vector Space Model\n",
        "\n",
        "This tutorial is a walk-through for the implementation of the vector space model (VSM) for text retrieval.\n",
        "* You should copy the dataset (zip file) containing 22,000 text files to your  VM.\n",
        "* You can mount your google drive on the VM and copy the dataset to your own google drive.\n",
        "* You will implement the VSM by following the step in this notebook.\n",
        "* After the VSM is set, you can give a query and the model should return the top five documents related to the query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChutGUrV8qQm",
        "outputId": "5b1e3275-0fd1-4560-c9bf-b8527d381fe6"
      },
      "source": [
        "# Copy the dataset to the VM file system.\n",
        "!wget https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip documents.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-29 00:28:46--  https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128, 108.177.121.128, 142.250.103.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226023 (221K) [application/x-zip-compressed]\n",
            "Saving to: ‘documents.zip’\n",
            "\n",
            "\rdocuments.zip         0%[                    ]       0  --.-KB/s               \rdocuments.zip       100%[===================>] 220.73K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-10-29 00:28:46 (94.4 MB/s) - ‘documents.zip’ saved [226023/226023]\n",
            "\n",
            "--2021-10-29 00:28:46--  http://documents.zip/\n",
            "Resolving documents.zip (documents.zip)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘documents.zip’\n",
            "FINISHED --2021-10-29 00:28:46--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 1 files, 221K in 0.002s (94.4 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqzAPOgO9dQv"
      },
      "source": [
        "Unzip the text document dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZzylvqDzBC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3151f5-1900-484c-a326-eea87503e03c"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = '/content/documents.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r',) as zip:\n",
        "  zip.extractall()\n",
        "  print('Done!!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olrQF1eM1Ts8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96119ead-a739-4b9a-94f4-4d44d128bae0"
      },
      "source": [
        "# uncomment this code if you want to mount your google drive to the VM\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gpvAXO4LZuI"
      },
      "source": [
        "Create SQL database (SQLite) to save all the table\n",
        "* SQLite stores everything in a single file\n",
        "* SQLite is the most convenient SQL DB solution for mobile App and Cloud computing (given the dataset is small)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv4ngG72aHKK"
      },
      "source": [
        "**Bellow cell imports all the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkFO0aH-8W5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ccd905-13ca-4b16-9aa1-f49ae6ae893d"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('popular');\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJd9bSh7aRn5"
      },
      "source": [
        "# Parse the text document dataset into a dictionary\n",
        "\n",
        "The get_docDict() function takes the dataset folder path\n",
        "\n",
        "removes the extra \"\\n\" end of line symbols\n",
        "\n",
        "returns a dictionary structure {filename : text content}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeT-JtmRlz7h"
      },
      "source": [
        "def get_docDict(path):\n",
        "  doc_dict = {}\n",
        "  file_names = os.listdir(path)\n",
        "\n",
        "  for file in file_names:\n",
        "    full_path = path+'/'+file\n",
        "    with open(full_path, 'r', errors='ignore') as f:\n",
        "      data = f.readlines()\n",
        "    text = \"\".join([i for i in data])\n",
        "    # remove all the \"\\n\" from the text\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    doc_dict[file] = text\n",
        "  return doc_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vI-BIyJRq5A",
        "outputId": "7d188993-b467-4e5f-b95a-b456368ea479"
      },
      "source": [
        "path = '/content/documents'\n",
        "\n",
        "doc_dict = get_docDict(path)\n",
        "doc_dict.keys()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['A00-1012.pdf.txt', 'A00-1000.pdf.txt', 'A00-1010.pdf.txt', 'A00-1014.pdf.txt', 'A00-1004.pdf.txt', 'A00-1007.pdf.txt', 'A00-1016.pdf.txt', 'A00-1006.pdf.txt', 'A00-1005.pdf.txt', 'A00-1017.pdf.txt', 'A00-1020.pdf.txt', 'A00-1008.pdf.txt', 'A00-1002.pdf.txt', 'A00-1013.pdf.txt', 'A00-1015.pdf.txt', 'A00-1003.pdf.txt', 'A00-1018.pdf.txt', 'A00-1001.pdf.txt', 'A00-1009.pdf.txt', 'A00-1011.pdf.txt', 'A00-1019.pdf.txt'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2f2SMGJaVnO"
      },
      "source": [
        "# Clean the text\n",
        "The clean_text() function perform the following tasks:\n",
        "* remove extra white space\n",
        "* remove extra dots \"...\" between lines in original document text\n",
        "* remove extra hyphen\n",
        "* tokenize: text string ==> a list of tokens\n",
        "* remove stop words and punctuation (English)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsYLF3lVk2iK"
      },
      "source": [
        "def clean_text(doc_dict):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : text}\n",
        "  output - a dictionary of {filename : clean text} \n",
        "\n",
        "  \"\"\"\n",
        "  clean_dict = {}\n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  \n",
        "  for name, doc in doc_dict.items():\n",
        "    # remove extra white space\n",
        "    text = re.sub(r\"\\s+\", \" \", doc)\n",
        "    # remove extra ...\n",
        "    text = re.sub(r\"\\.+\",\" \", doc)\n",
        "    # remove hyphen\n",
        "    text = re.sub(r\"-\",\"\", text)\n",
        "    text = text.lower()\n",
        "    text_tokens = word_tokenize(text)\n",
        "    text_clean = []\n",
        "    for word in text_tokens:\n",
        "      if (word not in stopwords_english and word not in string.punctuation):\n",
        "        # stem_word = stemmer.stem(word)\n",
        "        text_clean.append(word)\n",
        "    \n",
        "    clean_dict[name] = text_clean\n",
        "    \n",
        "  return clean_dict"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeB7zv9uh_em"
      },
      "source": [
        "clean_dict = clean_text(doc_dict)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJRTz1dpkQZt",
        "outputId": "822ac109-5765-4647-9d93-fc6ef4435ff3"
      },
      "source": [
        "clean_dict['A00-1000.pdf.txt'][:5]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['association', 'computational', 'linguistics', '6', 'th']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4qq75WF3HOs"
      },
      "source": [
        "# Make the vocabulary of whole document dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WDu9CjM3FQW"
      },
      "source": [
        "def make_vocab(doc_dict):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text} \n",
        "  output - a set of unique terms forms the dataset vocabulary\n",
        "  \"\"\"\n",
        "  total_tokens = []\n",
        "  for tokens in doc_dict.values():\n",
        "    total_tokens += tokens\n",
        "  vocab = list(set(total_tokens))\n",
        "  return vocab"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7e0jm656vYw",
        "outputId": "b8bc8b95-8da3-4d7e-9bb1-aae1d2dd38ee"
      },
      "source": [
        "vocab = make_vocab(clean_dict)\n",
        "len(vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10811"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDqAbaxRz0zu"
      },
      "source": [
        "# Calculate term frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkjTguyBzuhU"
      },
      "source": [
        "def get_DocTF(doc_dict, vocab):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text}, the vocabulary of the whole dataset\n",
        "  output - a dictionary of {filename : {term : count}}\n",
        "  \"\"\"\n",
        "  tf_dict = {}\n",
        "  # make the dict for filename=>{term:frequency}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_dict[doc_id] = {}\n",
        "\n",
        "  for word in vocab:\n",
        "    for doc_id, text in doc_dict.items():\n",
        "      tf_dict[doc_id][word] = text.count(word)\n",
        "    \n",
        "  return tf_dict"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyB6P_Ou2Dht"
      },
      "source": [
        "tf_dict = get_DocTF(clean_dict, vocab)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6gSs515_iAp",
        "outputId": "17a706d5-c8b9-4061-fdfc-8ee2cc0d1785"
      },
      "source": [
        "tf_dict['A00-1000.pdf.txt']['language']"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJ9xPqG_w0M"
      },
      "source": [
        "# Calculate document frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZqY34xJ1DF-"
      },
      "source": [
        "def get_DocDF(clean_dict, vocab):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text}, the vocabulary of the whole dataset\n",
        "  output - a dictionary of all terms in the vocabulary - {term : count}\n",
        "  \"\"\"\n",
        "  df_dict = {}\n",
        "  for word in vocab:\n",
        "    freq = 0\n",
        "    for text_tokens in clean_dict.values():\n",
        "      if word in text_tokens:\n",
        "        freq += 1\n",
        "    df_dict[word] = freq\n",
        "\n",
        "  return df_dict"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppSNsZD_zuoD"
      },
      "source": [
        "df_dict = get_DocDF(clean_dict, vocab)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8C2tLSYBTtz",
        "outputId": "bb0e4ee4-9a47-41b9-f4fd-1d0726f4a1f9"
      },
      "source": [
        "df_dict['language']"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYU55xW2Bvpz"
      },
      "source": [
        "# Calculate inverse document frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxmwGJ8V9Wco"
      },
      "source": [
        "def inverse_DF(df_dict, vocab, doc_length):\n",
        "  \"\"\"\n",
        "  input - a dictionary of DF {term : count}, the vocabulary of the whole dataset, total # of documents in the dataset\n",
        "  output - a dictionary of IDF of all terms in the vocabulary - {term : inver_df}\n",
        "  \"\"\"\n",
        "  idf_dict = {}\n",
        "  for word in vocab:\n",
        "    # idf_dict[word] = - np.log2((df_dict[word]) / (doc_length)) \n",
        "    idf_dict[word] = round(np.log(((doc_length - df_dict[word]+0.5) / (df_dict[word]+0.5))+1), 4)\n",
        "    \n",
        "  return idf_dict\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R01oM-2uSA15"
      },
      "source": [
        "doc_length = len(tf_dict.keys())\n",
        "idf_dict = inverse_DF(df_dict, vocab, doc_length)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK9hs9-9j9XC",
        "outputId": "b602efea-126a-4ed9-a3ea-70c9efa72e1c"
      },
      "source": [
        "idf_dict['text']"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1733"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9CTTmCUHPXs"
      },
      "source": [
        "# Calculate TF-IDF\n",
        "\n",
        "A term t in a given document d, TF-IDF(t,d) = TF(t,d) * IDF(t)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxWEl-dHHTpc"
      },
      "source": [
        "def get_tf_idf(tf_dict, idf_dict, doc_dict, vocab):\n",
        "  tf_idf_dict = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_idf_dict[doc_id] = {}\n",
        "  \n",
        "  for word in vocab:\n",
        "    for doc_id, text_tokens in doc_dict.items():\n",
        "      tf_idf_dict[doc_id][word] = round((tf_dict[doc_id][word] * idf_dict[word]), 4)\n",
        "  return tf_idf_dict"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX-8wAh9HS80"
      },
      "source": [
        "tf_idf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dm1wK8YJoP9",
        "outputId": "2d2b4f1b-9f9b-4113-a18f-8ec1ed3c6272"
      },
      "source": [
        "tf_idf_dict['A00-1001.pdf.txt']['text']"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3466"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdHpP87kTHgb"
      },
      "source": [
        "# Define the Vector Space Model (VSM)\n",
        "\n",
        "To find the relevant documents related to query, pass the query to function along with collection of documents (dictionary) and tf-idf scores (dictionary returned by tfidf). Function returns the top 5 documents from a collection of all documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlAP7DDhkX_K"
      },
      "source": [
        "def vectorSpaceModel(query, doc_dict,tfidf_dict):\n",
        "  query_vocab = []\n",
        "  query = query.lower()\n",
        "  query = re.sub(r\"\\s+\", \" \", query)\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  for word in query.split():\n",
        "    if (word not in string.punctuation and word not in stopwords_english):\n",
        "        query_vocab.append(word)\n",
        "\n",
        "  query_wc = {}\n",
        "  for word in query_vocab:\n",
        "    query_wc[word] = query.split().count(word)\n",
        "\n",
        "  relevance_scores = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    score = 0\n",
        "    for word in query_vocab:\n",
        "      score += query_wc[word] * tf_idf_dict[doc_id][word]\n",
        "    relevance_scores[doc_id] = round(score,4)\n",
        "\n",
        "  # sort the relevance score and get the top-k ranking\n",
        "  # sort the keys of the relevance score by value\n",
        "  sort_keys = sorted(relevance_scores, key=relevance_scores.get , reverse = True)\n",
        "  top_keys = sort_keys[:5]\n",
        "  top_5 = {}\n",
        "  for key in top_keys:\n",
        "    top_5[key] = relevance_scores[key]\n",
        "\n",
        "  return top_5\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcVbE98SAECH"
      },
      "source": [
        "# Test the VSM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s0A16tgAM5m",
        "outputId": "a64639cc-1b6f-4968-8031-4daf1f088667"
      },
      "source": [
        "# get the text documents\n",
        "path = '/content/documents'\n",
        "doc_dict = get_docDict(path)\n",
        "\n",
        "# clean the text\n",
        "clean_dict = clean_text(doc_dict)\n",
        "\n",
        "# get the vocabulary of the whole dataset\n",
        "vocab = make_vocab(clean_dict)\n",
        "\n",
        "# get the term frequency (TF)\n",
        "tf_dict = get_DocTF(clean_dict, vocab)\n",
        "\n",
        "# get the document frequency (DF)\n",
        "df_dict = get_DocDF(clean_dict, vocab)\n",
        "\n",
        "# get the inverse document frequency (IDF)\n",
        "doc_length = len(tf_dict.keys())\n",
        "idf_dict = inverse_DF(df_dict, vocab, doc_length)\n",
        "\n",
        "# calculate TF-IDF\n",
        "tf_idf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab)\n",
        "\n",
        "query1 = \"Natural Language\"\n",
        "result1 = vectorSpaceModel(query1, doc_dict,tf_idf_dict)\n",
        "print(result1)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A00-1001.pdf.txt': 2.294, 'A00-1007.pdf.txt': 1.849, 'A00-1005.pdf.txt': 1.2414, 'A00-1016.pdf.txt': 0.951, 'A00-1009.pdf.txt': 0.9114}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stLKl0z44jRS",
        "outputId": "4742ba5e-8e39-46a3-f113-8e5e6ef0d2ff"
      },
      "source": [
        "query2 = \"Data mining\"\n",
        "result2 = vectorSpaceModel(query2, doc_dict,tf_idf_dict)\n",
        "\n",
        "query3 = \"I like text retrieval\"\n",
        "result3 = vectorSpaceModel(query3, doc_dict,tf_idf_dict)\n",
        "\n",
        "query4 = \"probability model and language model\"\n",
        "result4 = vectorSpaceModel(query4, doc_dict,tf_idf_dict)\n",
        "\n",
        "print(result2)\n",
        "print()\n",
        "print(result3)\n",
        "print()\n",
        "print(result4)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A00-1004.pdf.txt': 21.748, 'A00-1020.pdf.txt': 6.334, 'A00-1017.pdf.txt': 5.199, 'A00-1005.pdf.txt': 1.5597, 'A00-1009.pdf.txt': 0.8665}\n",
            "\n",
            "{'A00-1003.pdf.txt': 60.3656, 'A00-1012.pdf.txt': 12.483, 'A00-1004.pdf.txt': 10.4346, 'A00-1018.pdf.txt': 7.1472, 'A00-1020.pdf.txt': 4.8524}\n",
            "\n",
            "{'A00-1004.pdf.txt': 90.612, 'A00-1019.pdf.txt': 63.0706, 'A00-1007.pdf.txt': 13.7778, 'A00-1012.pdf.txt': 12.0552, 'A00-1017.pdf.txt': 11.8802}\n"
          ]
        }
      ]
    }
  ]
}