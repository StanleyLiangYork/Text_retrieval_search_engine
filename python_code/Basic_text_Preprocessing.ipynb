{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_text_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates the basic steps of text preprocessing.\n",
        "\n",
        "\n",
        "*   converting all letters to lower or upper case\n",
        "*   converting numbers into words or removing numbers\n",
        "*   removing punctuations, accent marks and other diacritics\n",
        "*   removing white spaces\n",
        "*   expanding abbreviations\n",
        "*   removing stop words, sparse terms, and particular words\n",
        "*   text canonicalization\n"
      ],
      "metadata": {
        "id": "vaC5AMGXyO_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert text to lowercase\n",
        "simply use the standard python string (str) package "
      ],
      "metadata": {
        "id": "atRzc43Zywrq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK8yF2yYyEor",
        "outputId": "971695d1-ffc9-4952-b441-5b59bf524ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 5 biggest countries by population in this world are china, india, united states, indonesia, and brazil.\n"
          ]
        }
      ],
      "source": [
        "input_str = \"The 5 biggest countries by population in this world are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = input_str.lower()\n",
        "print(input_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove numbers\n",
        "use the regex (re) package to remove numbers"
      ],
      "metadata": {
        "id": "t4TUOerazOGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_str = \" A sample space contains 3 red balls, 5 white balls, 7 pink balls, and 2 blue balls.\"\n",
        "result = re.sub(r'\\d+', '', input_str)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AES3oQJ1UNl",
        "outputId": "47a0db89-c410-4c49-926a-8e6f657d9066"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A sample space contains  red balls,  white balls,  pink balls, and  blue balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove punctuation\n",
        "use the string package to remove a predefined set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"
      ],
      "metadata": {
        "id": "dmQrPIxX18a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "input_str = \"This &is [an] example? {of} string. with.?+-*=%& many ?@ punctuation!!!!\"\n",
        "# result = input_str.translate(str.maketrans(string.punctuation,\"\"))\n",
        "result = [c for c in input_str if c not in string.punctuation]\n",
        "result = ''.join(result)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w55anIZY1ztO",
        "outputId": "95dc3397-cd16-4508-8840-547dbfd9f222"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of string with many  punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove whitespaces"
      ],
      "metadata": {
        "id": "N1V7YAM9AXLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \" \\t a string example with white space \"\n",
        "print(f'Before whitespace removed: {input_str}')\n",
        "input_str = input_str.strip()\n",
        "print(f'After whitespace removed: {input_str}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWKn2MdL-eSn",
        "outputId": "5e67e102-fcf1-4ceb-b777-fbaf5ee606c4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before whitespace removed:  \t a string example with white space \n",
            "After whitespace removed: a string example with white space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove stop words\n",
        "**Stop words** are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK)"
      ],
      "metadata": {
        "id": "AfLC_vj3BUyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words= stopwords.words('english')\n",
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "tokens = word_tokenize(input_str)\n",
        "print(f'Before removing: {tokens}')\n",
        "result = [i for i in tokens if not i in stop_words]\n",
        "print(f'After removing: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vADDDnAZ-O-m",
        "outputId": "2dacc9e7-a0ff-4578-863e-febb2ccb1a5d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Before removing: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
            "After removing: ['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part of speech tagging (POS)\n",
        "POS aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context."
      ],
      "metadata": {
        "id": "VTdzK0A1E_Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "input_str = \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
        "tokens = word_tokenize(input_str)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGrfEcFCE46W",
        "outputId": "70ea75c9-ba37-4572-d0c1-982a451a0b89"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), (':', ':'), ('an', 'DT'), ('article', 'NN'), (',', ','), ('to', 'TO'), ('write', 'VB'), (',', ','), ('interesting', 'VBG'), (',', ','), ('easily', 'RB'), (',', ','), ('and', 'CC'), (',', ','), ('of', 'IN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking (shallow parsing)\n",
        "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) "
      ],
      "metadata": {
        "id": "Af5HYV9_HCGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, we add pos tags to the text - define the part of the speech for each word\n",
        "input_str = \"A black television and a white stove were bought for the new apartment of John.\"\n",
        "tokens = word_tokenize(input_str)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# second, chunking the text\n",
        "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "rp = nltk.RegexpParser(reg_exp)\n",
        "result = rp.parse(pos_tags)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc_0VfrWHHME",
        "outputId": "13da5244-4e17-4993-8439-fffdd7a24b12"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP A/DT black/JJ television/NN)\n",
            "  and/CC\n",
            "  (NP a/DT white/JJ stove/NN)\n",
            "  were/VBD\n",
            "  bought/VBN\n",
            "  for/IN\n",
            "  (NP the/DT new/JJ apartment/NN)\n",
            "  of/IN\n",
            "  John/NNP\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named entity recognition\n",
        "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
      ],
      "metadata": {
        "id": "03V9PoloIYwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
        "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdeDDfRVIYjF",
        "outputId": "996ce4b2-00d3-4dc0-debc-4c5cba56dae6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  Apple/NNP\n",
            "  so/IN\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Boston/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  conference/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}