{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQLite_text_retrieval.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ttygGuHYkQ"
      },
      "source": [
        "This notebook is a walk-through for loading the text documents and TF-IDF table from a SQLite database and run the text retrieval applications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZKgnX1xyEZX",
        "outputId": "ae586b01-e1fd-4de6-9757-2680130b1bbc"
      },
      "source": [
        "# Copy the dataset to the VM file system.\n",
        "!wget https://storage.googleapis.com/pet-detect-239118/text_retrieval/textDoc.db textDoc.db"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-29 02:31:21--  https://storage.googleapis.com/pet-detect-239118/text_retrieval/textDoc.db\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.197.128, 64.233.191.128, 173.194.74.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8753152 (8.3M) [application/octet-stream]\n",
            "Saving to: ‘textDoc.db’\n",
            "\n",
            "textDoc.db          100%[===================>]   8.35M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-10-29 02:31:22 (101 MB/s) - ‘textDoc.db’ saved [8753152/8753152]\n",
            "\n",
            "--2021-10-29 02:31:22--  http://textdoc.db/\n",
            "Resolving textdoc.db (textdoc.db)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘textdoc.db’\n",
            "FINISHED --2021-10-29 02:31:22--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 1 files, 8.3M in 0.08s (101 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGI26w0541uy",
        "outputId": "2884c55c-3958-4cdd-b275-0fc33eb14f29"
      },
      "source": [
        "import sqlite3\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('popular');\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZYVax4U5D5J"
      },
      "source": [
        "Load the data from the SQL database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec4HxoPs5Haj",
        "outputId": "2320ab0f-fd2e-4695-e627-71be3cf5a2f1"
      },
      "source": [
        "conn = sqlite3.connect('textDoc.db') \n",
        "cur = conn.cursor()\n",
        "print(\"Opened database successfully\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opened database successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT-Ubhv56ewq"
      },
      "source": [
        "doc_dict ={}\n",
        "for row in cur.execute('SELECT doc_name, raw_text FROM raw_data'):\n",
        "  doc_dict[row[0]] = row[1] "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzdjY6EZ-pw0"
      },
      "source": [
        "tf_idf_dict = {}\n",
        "\n",
        "for row in cur.execute('SELECT DISTINCT doc_name FROM tf_idf'):\n",
        "  tf_idf_dict[row[0]] = {}\n",
        "\n",
        "for row in cur.execute('SELECT doc_name, term, score from tf_idf'):\n",
        "  for doc_id in tf_idf_dict.keys():\n",
        "    if doc_id==row[0]:\n",
        "      tf_idf_dict[doc_id][row[1]] = row[2]\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOtsAGoy6Oaz"
      },
      "source": [
        "def vectorSpaceModel(query, doc_dict,tfidf_dict):\n",
        "  query_vocab = []\n",
        "  query = query.lower()\n",
        "  query = re.sub(r\"\\s+\", \" \", query)\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  for word in query.split():\n",
        "    if (word not in string.punctuation and word not in stopwords_english):\n",
        "        query_vocab.append(word)\n",
        "\n",
        "  query_wc = {}\n",
        "  for word in query_vocab:\n",
        "    query_wc[word] = query.split().count(word)\n",
        "\n",
        "  relevance_scores = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    score = 0\n",
        "    for word in query_vocab:\n",
        "      score += query_wc[word] * tf_idf_dict[doc_id][word]\n",
        "    relevance_scores[doc_id] = round(score,4)\n",
        "\n",
        "  # sort the relevance score and get the top-k ranking\n",
        "  # sort the keys of the relevance score by value\n",
        "  sort_keys = sorted(relevance_scores, key=relevance_scores.get , reverse = True)\n",
        "  top_keys = sort_keys[:5]\n",
        "  top_5 = {}\n",
        "  for key in top_keys:\n",
        "    top_5[key] = relevance_scores[key]\n",
        "\n",
        "  return top_5\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-kWz3OJ6a8K",
        "outputId": "41af54be-4440-476c-c7d2-0ef9b82cee65"
      },
      "source": [
        "query1 = \"Natural Language\"\n",
        "result1 = vectorSpaceModel(query1, doc_dict,tf_idf_dict)\n",
        "print(result1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A00-1001.pdf.txt': 2.294, 'A00-1007.pdf.txt': 1.849, 'A00-1005.pdf.txt': 1.2414, 'A00-1016.pdf.txt': 0.951, 'A00-1009.pdf.txt': 0.9114}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViPYzR6uD24G",
        "outputId": "2aeaf980-5d11-46ba-b1e7-8d568dd122ca"
      },
      "source": [
        "query2 = \"Data mining\"\n",
        "result2 = vectorSpaceModel(query2, doc_dict,tf_idf_dict)\n",
        "\n",
        "query3 = \"I like text retrieval\"\n",
        "result3 = vectorSpaceModel(query3, doc_dict,tf_idf_dict)\n",
        "\n",
        "query4 = \"probability model and language model\"\n",
        "result4 = vectorSpaceModel(query4, doc_dict,tf_idf_dict)\n",
        "\n",
        "print(result2)\n",
        "print()\n",
        "print(result3)\n",
        "print()\n",
        "print(result4)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A00-1004.pdf.txt': 21.748, 'A00-1020.pdf.txt': 6.334, 'A00-1017.pdf.txt': 5.199, 'A00-1005.pdf.txt': 1.5597, 'A00-1009.pdf.txt': 0.8665}\n",
            "\n",
            "{'A00-1003.pdf.txt': 60.3656, 'A00-1012.pdf.txt': 12.483, 'A00-1004.pdf.txt': 10.4346, 'A00-1018.pdf.txt': 7.1472, 'A00-1020.pdf.txt': 4.8524}\n",
            "\n",
            "{'A00-1004.pdf.txt': 90.612, 'A00-1019.pdf.txt': 63.0706, 'A00-1007.pdf.txt': 13.7778, 'A00-1012.pdf.txt': 12.0552, 'A00-1017.pdf.txt': 11.8802}\n"
          ]
        }
      ]
    }
  ]
}