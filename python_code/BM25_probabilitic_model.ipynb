{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "ml_test",
      "display_name": "ML test",
      "language": "python"
    },
    "colab": {
      "name": "BM25_probabilitic_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UOXBaKKZ1-R"
      },
      "source": [
        "This notebook is a work through for the implementation of a basic BM25 algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JS75rVkWymn",
        "outputId": "9647d30d-0e1c-41f8-dec3-4060478220e6"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('popular');\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOVlkexvYpzh"
      },
      "source": [
        "Get the text document collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvDE543nWyvG",
        "outputId": "7930bd0b-0dde-47c3-99db-2b3d3ceff5b2"
      },
      "source": [
        "!wget https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip documents.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-30 00:52:50--  https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 74.125.199.128, 74.125.20.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226023 (221K) [application/x-zip-compressed]\n",
            "Saving to: ‘documents.zip’\n",
            "\n",
            "\rdocuments.zip         0%[                    ]       0  --.-KB/s               \rdocuments.zip       100%[===================>] 220.73K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-10-30 00:52:50 (99.1 MB/s) - ‘documents.zip’ saved [226023/226023]\n",
            "\n",
            "--2021-10-30 00:52:50--  http://documents.zip/\n",
            "Resolving documents.zip (documents.zip)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘documents.zip’\n",
            "FINISHED --2021-10-30 00:52:51--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 1 files, 221K in 0.002s (99.1 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSgYkTCTYwwp",
        "outputId": "48a225ea-7301-43de-a794-f53fe633a90e"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = '/content/documents.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r',) as zip:\n",
        "  zip.extractall()\n",
        "  print('Done!!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxfxN846ZF1I"
      },
      "source": [
        "Parse the text document dataset into a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9kxgLaMWy0f"
      },
      "source": [
        "def get_docDict(path):\n",
        "  doc_dict = {}\n",
        "  file_names = os.listdir(path)\n",
        "\n",
        "  for file in file_names:\n",
        "    full_path = path+'/'+file\n",
        "    with open(full_path, 'r', errors='ignore') as f:\n",
        "      data = f.readlines()\n",
        "    text = \"\".join([i for i in data])\n",
        "    # remove all the \"\\n\" from the text\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    doc_dict[file] = text\n",
        "  return doc_dict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hGWHbmKY_5h"
      },
      "source": [
        "path = '/content/documents'\n",
        "doc_dict = get_docDict(path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kyTa9QVZTfz"
      },
      "source": [
        "Clean the text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDNaRfovY_80"
      },
      "source": [
        "def clean_text(doc_dict):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : text}\n",
        "  output - a dictionary of {filename : clean text} \n",
        "\n",
        "  \"\"\"\n",
        "  clean_dict = {}\n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  \n",
        "  for name, doc in doc_dict.items():\n",
        "    # remove extra white space\n",
        "    text = re.sub(r\"\\s+\", \" \", doc)\n",
        "    # remove extra ...\n",
        "    text = re.sub(r\"\\.+\",\" \", doc)\n",
        "    # remove hyphen\n",
        "    text = re.sub(r\"-\",\"\", text)\n",
        "    text = text.lower()\n",
        "    text_tokens = word_tokenize(text)\n",
        "    text_clean = []\n",
        "    for word in text_tokens:\n",
        "      if (word not in stopwords_english and word not in string.punctuation):\n",
        "        stem_word = stemmer.stem(word)\n",
        "        text_clean.append(word)\n",
        "    \n",
        "    clean_dict[name] = text_clean\n",
        "    \n",
        "  return clean_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxZX1WYwZjOo",
        "outputId": "a62fa199-de7b-4dc2-ebca-2d4c0815116c"
      },
      "source": [
        "clean_dict = clean_text(doc_dict)\n",
        "clean_dict['A00-1000.pdf.txt'][:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['association', 'computational', 'linguistics', '6', 'th']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyApw2PYZ1Xo"
      },
      "source": [
        "Build the dataset vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLEnR5RjZxEp"
      },
      "source": [
        "def make_vocab(doc_dict):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text} \n",
        "  output - a set of unique terms forms the dataset vocabulary\n",
        "  \"\"\"\n",
        "  total_tokens = []\n",
        "  for tokens in doc_dict.values():\n",
        "    total_tokens += tokens\n",
        "  vocab = list(set(total_tokens))\n",
        "  return vocab"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndlNZUEMaPWE",
        "outputId": "00dc0a23-7e73-41b5-f15c-8d9f95ff18e2"
      },
      "source": [
        "vocab = make_vocab(clean_dict)\n",
        "len(vocab)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10811"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eG-AxbXaZno"
      },
      "source": [
        "Calculate TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMAGxcJDaTRj"
      },
      "source": [
        "# compute document-wise term frequency\n",
        "\n",
        "def get_DocTF(doc_dict, vocab):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text}, the vocabulary of the whole dataset\n",
        "  output - a dictionary of {filename : {term : count}}\n",
        "  \"\"\"\n",
        "  tf_dict = {}\n",
        "  # make the dict for filename=>{term:frequency}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_dict[doc_id] = {}\n",
        "\n",
        "  for word in vocab:\n",
        "    for doc_id, text in doc_dict.items():\n",
        "      tf_dict[doc_id][word] = text.count(word)\n",
        "    \n",
        "  return tf_dict"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMi_QZzpav1y"
      },
      "source": [
        "tf_dict = get_DocTF(clean_dict, vocab)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AgEsfqJa0QC"
      },
      "source": [
        "# compute document frequency\n",
        "\n",
        "def get_DocDF(clean_dict, vocab):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text}, the vocabulary of the whole dataset\n",
        "  output - a dictionary of all terms in the vocabulary - {term : count}\n",
        "  \"\"\"\n",
        "  df_dict = {}\n",
        "  for word in vocab:\n",
        "    freq = 0\n",
        "    for text_tokens in clean_dict.values():\n",
        "      if word in text_tokens:\n",
        "        freq += 1\n",
        "    df_dict[word] = freq\n",
        "\n",
        "  return df_dict"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsQ6w5gIbQ-1"
      },
      "source": [
        "df_dict = get_DocDF(clean_dict, vocab)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY76XOqzZjXo"
      },
      "source": [
        "# compute IDF\n",
        "\n",
        "def inverse_DF(df_dict, vocab, N):\n",
        "  \"\"\"\n",
        "  input - a dictionary of DF {term : count}, the vocabulary of the whole dataset, total # of documents in the dataset\n",
        "  output - a dictionary of IDF of all terms in the vocabulary - {term : inver_df}\n",
        "  \"\"\"\n",
        "  idf_dict = {}\n",
        "\n",
        "  for word in vocab:\n",
        "    N_q = df_dict[word]\n",
        "    # idf_dict[word] = np.log10(N / N_q) \n",
        "    idf_dict[word] = np.log(((N - N_q + 0.5) / (N_q + 0.5)) + 1)\n",
        "       \n",
        "  return idf_dict"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1Eg6Icjb1DM"
      },
      "source": [
        "N = len(tf_dict.keys())\n",
        "idf_dict = inverse_DF(df_dict, vocab, N)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmcRNCAZcObE",
        "outputId": "d5ce5482-0833-4da5-b8e6-73eb65573bba"
      },
      "source": [
        "idf_dict['language']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02298951822469878"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "squ7X6bbiaoW"
      },
      "source": [
        "# compute the TF-IDF dictionary \n",
        "\n",
        "def get_tf_idf(tf_dict, idf_dict, doc_dict, vocab):\n",
        "  tf_idf_dict = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_idf_dict[doc_id] = {}\n",
        "  \n",
        "  for word in vocab:\n",
        "    for doc_id, text_tokens in doc_dict.items():\n",
        "      tf_idf_dict[doc_id][word] = round((tf_dict[doc_id][word] * idf_dict[word]), 4)\n",
        "  return tf_idf_dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXyPWXF0ioId",
        "outputId": "223cf553-d6de-42fa-9a60-c7b07ce91949"
      },
      "source": [
        "tf_idf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab)\n",
        "tf_idf_dict['A00-1001.pdf.txt']['language']"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5288"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWScQ3RvZ1-0"
      },
      "source": [
        "To use the BM25 algorithm, we still need to compute TF and IDF. \n",
        "\n",
        "The **TF** component includes the query frequency, and total number of terms as with TF-IDF, but we add two special parameters `k` and `b` which we can use to optimize our algorithm, but by default we stick with the values of `1.2` and `0.75` respectively. We also include two new values:\n",
        "* Average document length (avgdl) - the average length of *all* documents\n",
        "* *D* - the length of the current document.\n",
        "\n",
        "Our new **IDF** component is similar, it maintains the parametes `N` and `N_q`, which are the number of documents and the number of documents *that contain* our query, respectively. And all we do is add a few values (0.5, 1) here and there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps-Cz_5gdtBe"
      },
      "source": [
        "# compute average document length (use the cleaned text)\n",
        "\n",
        "def get_avgdl(clean_dict):\n",
        "  total_doc = len(clean_dict.keys())\n",
        "  total_length = 0\n",
        "  for text in clean_dict.values():\n",
        "    total_length += len(text)\n",
        "\n",
        "  avgdl = total_length / total_doc\n",
        "\n",
        "  return round(avgdl, 4)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3Hku6jhgRkY",
        "outputId": "c9be1d24-b7d9-4792-e8cd-5c4db501c310"
      },
      "source": [
        "avgdl = get_avgdl(clean_dict)\n",
        "avgdl"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2764.3333"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAInlrw2w2pq"
      },
      "source": [
        "# Implement the BM25 algorithm\n",
        "* compute a dictionary of a score of a given term from a given document\n",
        "* the total score of a given document is presented as\n",
        "\\begin{align}\n",
        "    BM25(d)=\\sum_{t=q, f_t,_d >0} log(1 + \\frac{N-df_t+0.5}{df_t+0.5}) \\cdot \\frac{f_t,_d}{f_t,_d + K \\cdot (1-b+b\\frac{l(d)}{avgdl}) }\n",
        "\\end{align}\n",
        "\n",
        "*d* - document, *t* - term, *q* - word from the query, *N* - total number of documents\n",
        "\n",
        "$f_t,_d$ - term frequenct (TF) given a word from query, $df_t$ - document frequency of a given term\n",
        "\n",
        "$l(d)$ - total length of a given document d, $avgdl$ - average length of document in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXgPNA_FdtF-"
      },
      "source": [
        "# compute BM25 score dictionary\n",
        "\n",
        "def bm25(tf_dict, clean_dict, df_dict, vocab, k=1.2, b=0.75):\n",
        "  bm25_dict = {}\n",
        "  avgdl = get_avgdl(clean_dict)\n",
        "  N = len(clean_dict.keys())\n",
        "  \n",
        "  # create the collection of dictionaries of all documents\n",
        "  for doc_id in clean_dict.keys():\n",
        "    bm25_dict[doc_id] = {}\n",
        "\n",
        "  for word in vocab:\n",
        "    for doc_id, text_tokens in clean_dict.items():\n",
        "      freq = tf_dict[doc_id][word]\n",
        "      # the TF in BM25\n",
        "      tf = (freq*(k+1)) / (freq + k*(1-b+b*len(clean_dict[doc_id])/avgdl))\n",
        "      # get DF\n",
        "      N_q = df_dict[word]\n",
        "      idf = np.log(((N - N_q + 0.5) / (N_q + 0.5)) + 1)\n",
        "      score = round(tf*idf,4)\n",
        "      bm25_dict[doc_id][word] = score\n",
        "\n",
        "  return bm25_dict"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxK1CrE6iN3e"
      },
      "source": [
        "bm25_dict = bm25(tf_dict, clean_dict, df_dict, vocab)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzmBDpQxiN8A",
        "outputId": "d51af227-de00-44b8-e545-4d2b15690bbc"
      },
      "source": [
        "bm25_dict['A00-1020.pdf.txt']['language']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0415"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szEnkFE-qDpx"
      },
      "source": [
        "# Define the BM25 Model (Probabilistic Model)\n",
        "* To find the most relevant documents related to the query\n",
        "* Pass the query along with the document set (dictionary) and the BM25 scores of all words in the vocabulary\n",
        "* Returns the top 5 documents ID and scores \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC49NLyIqcAH"
      },
      "source": [
        "def BM25Model(query, doc_dict, bm25_dict):\n",
        "  query_vocab = []\n",
        "  query = query.lower()\n",
        "  query = re.sub(r\"\\s+\", \" \", query)\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  for word in query.split():\n",
        "    if (word not in string.punctuation and word not in stopwords_english):\n",
        "        query_vocab.append(word)\n",
        "\n",
        "  query_wc = {}\n",
        "  for word in query_vocab:\n",
        "    query_wc[word] = query.split().count(word)\n",
        "\n",
        "  relevance_scores = {}\n",
        "  # use the raw doc_dict to get the filename only\n",
        "  for doc_id in doc_dict.keys():\n",
        "    score = 0\n",
        "    for word in query_vocab:\n",
        "      score += query_wc[word] * bm25_dict[doc_id][word]\n",
        "    relevance_scores[doc_id] = round(score,4)\n",
        "\n",
        "  # sort the relevance score and get the top-k ranking\n",
        "  # sort the keys of the relevance score by value\n",
        "  sort_keys = sorted(relevance_scores, key=relevance_scores.get , reverse = True)\n",
        "  top_keys = sort_keys[:5]\n",
        "  top_5 = {}\n",
        "  for key in top_keys:\n",
        "    top_5[key] = relevance_scores[key]\n",
        "\n",
        "  return top_5"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcg0Wnkt8hCK"
      },
      "source": [
        "The vector space model (VSM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC-kPXr_rl5k"
      },
      "source": [
        "def get_tf_idf(tf_dict, idf_dict, doc_dict, vocab):\n",
        "  tf_idf_dict = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_idf_dict[doc_id] = {}\n",
        "  \n",
        "  for word in vocab:\n",
        "    for doc_id, text_tokens in doc_dict.items():\n",
        "      tf_idf_dict[doc_id][word] = round((tf_dict[doc_id][word] * idf_dict[word]), 4)\n",
        "  return tf_idf_dict\n",
        "\n",
        "\n",
        "def vectorSpaceModel(query, doc_dict,tfidf_dict):\n",
        "  query_vocab = []\n",
        "  query = query.lower()\n",
        "  query = re.sub(r\"\\s+\", \" \", query)\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  for word in query.split():\n",
        "    if (word not in string.punctuation and word not in stopwords_english):\n",
        "        query_vocab.append(word)\n",
        "\n",
        "  query_wc = {}\n",
        "  for word in query_vocab:\n",
        "    query_wc[word] = query.split().count(word)\n",
        "\n",
        "  relevance_scores = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    score = 0\n",
        "    for word in query_vocab:\n",
        "      score += query_wc[word] * tf_idf_dict[doc_id][word]\n",
        "    relevance_scores[doc_id] = round(score,4)\n",
        "\n",
        "  # sort the relevance score and get the top-k ranking\n",
        "  # sort the keys of the relevance score by value\n",
        "  sort_keys = sorted(relevance_scores, key=relevance_scores.get , reverse = True)\n",
        "  top_keys = sort_keys[:5]\n",
        "  top_5 = {}\n",
        "  for key in top_keys:\n",
        "    top_5[key] = relevance_scores[key]\n",
        "\n",
        "  return top_5"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewZQ-go5_ZVr"
      },
      "source": [
        "tfidf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab) "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ChZtRZAqdDy",
        "outputId": "d70743eb-9a20-4d6c-8445-8445aeb07f0b"
      },
      "source": [
        "query1 = \"Natural Language\"\n",
        "result1 = BM25Model(query1, doc_dict, bm25_dict)\n",
        "print(f'ranked by BM25: {result1}')\n",
        "result2 = vectorSpaceModel(query1, doc_dict, tfidf_dict)\n",
        "print(f'ranked by VSM: {result2}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ranked by BM25: {'A00-1001.pdf.txt': 0.1977, 'A00-1007.pdf.txt': 0.1926, 'A00-1005.pdf.txt': 0.188, 'A00-1000.pdf.txt': 0.1856, 'A00-1009.pdf.txt': 0.1831}\n",
            "ranked by VSM: {'A00-1001.pdf.txt': 2.2942, 'A00-1007.pdf.txt': 1.8492, 'A00-1005.pdf.txt': 1.2415, 'A00-1016.pdf.txt': 0.9508, 'A00-1009.pdf.txt': 0.9115}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zDf4VgqcM_",
        "outputId": "426c349b-7092-471d-d0b5-cf21e5e4d94d"
      },
      "source": [
        "query2 = \"generative model\"\n",
        "result1 = BM25Model(query2, doc_dict, bm25_dict)\n",
        "print(f'ranked by BM25: {result1}')\n",
        "tfidf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab) \n",
        "result2 = vectorSpaceModel(query2, doc_dict, tfidf_dict)\n",
        "print(f'ranked by VSM: {result2}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ranked by BM25: {'A00-1010.pdf.txt': 3.7823, 'A00-1014.pdf.txt': 2.5833, 'A00-1004.pdf.txt': 0.8908, 'A00-1019.pdf.txt': 0.8835, 'A00-1007.pdf.txt': 0.7827}\n",
            "ranked by VSM: {'A00-1004.pdf.txt': 20.8447, 'A00-1019.pdf.txt': 15.0082, 'A00-1010.pdf.txt': 6.0171, 'A00-1014.pdf.txt': 4.2593, 'A00-1007.pdf.txt': 3.3352}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btVL1ARu9SDM",
        "outputId": "73d153ac-c9b0-4fa1-9c91-e86793ca2254"
      },
      "source": [
        "query3 = \"text retrieval for nature language\"\n",
        "result1 = BM25Model(query3, doc_dict, bm25_dict)\n",
        "print(f'ranked by BM25: {result1}')\n",
        "tfidf_dict = get_tf_idf(tf_dict, idf_dict, doc_dict, vocab) \n",
        "result2 = vectorSpaceModel(query3, doc_dict, tfidf_dict)\n",
        "print(f'ranked by VSM: {result2}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ranked by BM25: {'A00-1019.pdf.txt': 4.0482, 'A00-1004.pdf.txt': 3.9572, 'A00-1018.pdf.txt': 3.1224, 'A00-1003.pdf.txt': 3.0071, 'A00-1000.pdf.txt': 2.6395}\n",
            "ranked by VSM: {'A00-1003.pdf.txt': 60.3235, 'A00-1004.pdf.txt': 12.596, 'A00-1018.pdf.txt': 12.0305, 'A00-1012.pdf.txt': 11.9561, 'A00-1019.pdf.txt': 8.0488}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkf3YB0kCUqf"
      },
      "source": [
        "Visualize score versus document length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKAT2GdI_EXC"
      },
      "source": [
        "BM25_term_score = []\n",
        "VSM_term_score = []\n",
        "doc_length = []\n",
        "\n",
        "for doc_id in doc_dict.keys():\n",
        "  doc_length.append(len(clean_dict[doc_id]))\n",
        "  vsm_score = np.sum([x for x in tfidf_dict[doc_id].values()])\n",
        "  VSM_term_score.append(vsm_score)\n",
        "  bm25_score = np.sum([x for x in bm25_dict[doc_id].values()])\n",
        "  BM25_term_score.append(bm25_score)\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "fPBC_xbu98Gc",
        "outputId": "8a25c6ad-ff25-48ca-a214-86eccda63e23"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 8))\n",
        "data = [BM25_term_score, VSM_term_score]\n",
        "X = np.arange(21)\n",
        "\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(X + 0.00, data[0], color = 'r', width = 0.25)\n",
        "ax.bar(X + 0.25, data[1], color = 'g', width = 0.25)\n",
        "ax.legend(['BM25 score', 'VSM score'])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff0090f2190>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA44AAAJfCAYAAAAw4oiqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfbCdVWHv8d8iiQSESggxQxM00TJXAoSASYwjUEJGXvSOiYIRVKCgwHTC2FpawVYnUGUq7fhy6aU63AtjVCSotJgWK+VFjFJtCRCjEhm5CCWAEggFKQVNsu4f58mZBJKVE3Jewjmfz8yZ8+y1n/3stc/sOZNvnmevU2qtAQAAgG3ZbagnAAAAwK5NOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE2jh3oCLfvtt1+dMmXKUE8DAABg2Lvzzjsfr7VO2Np9u3Q4TpkyJStWrBjqaQAAAAx7pZQHt3WfS1UBAABoEo4AAAA0CUcAAACadunPOAIAAMPbb3/726xZsybPPffcUE9lxBg7dmwmT56cMWPG9PkxwhEAABgya9asyd57750pU6aklDLU0xn2aq154oknsmbNmkydOrXPj3OpKgAAMGSee+65jB8/XjQOklJKxo8fv8NneIUjAAAwpETj4HopP2/hCAAAjGijRo3KjBkzcthhh+WII47Iv/7rvyZJHnjggZRS8rGPfax338cffzxjxozJeeedlyT5zGc+k2nTpmX69OmZN29eHnzwwRcdd8aMGXnHO94xuC+qnwlHAABg11FK/371wR577JGVK1fmRz/6Uf7qr/4qH/3oR3vvmzp1am644Ybe21//+tdz8MEH994+/PDDs2LFiqxatSonn3xyPvKRj7zouCtXrsyyZcv64YezfRs2bBiQ4wpHAACAztNPP51x48b13t5zzz1z0EEHZcWKFUmSa6+9NgsXLuy9f+7cudlzzz2TJHPmzMmaNWt26Pkuu+yy3jOWp5xySpLkmWeeyZlnnplDDz0006dPz3XXXZckueaaa3LooYfmkEMOyQUXXNB7jL322ivnn39+DjvssPzgBz/IV77ylcyePTszZszIueee2y8xKRwBAIAR7b//+78zY8aMvOENb8gHP/jBfPzjH9/i/lNOOSVLly7NQw89lFGjRuV3f/d3t3qcK6+8MieeeGLv7eeeey4zZ87MnDlzcv3112/1MZ/61Kdy9913Z9WqVfnCF76QJPnEJz6RV73qVfnxj3+cVatW5dhjj80jjzySCy64ILfeemtWrlyZO+64o/eY//Vf/5U3velN+dGPfpTx48fn2muvze23356VK1dm1KhRufrqq3f6Z+TPcQAAACPapktKk+QHP/hBTj/99PzkJz/pvf+EE07Ixz/+8UycODHvec97tnqMr3zlK1mxYkW++93v9o49+OCDmTRpUu6///4ce+yxOfTQQ/P6179+i8dNnz4973vf+7JgwYIsWLAgSXLzzTdn6dKlvfuMGzcuy5cvzzHHHJMJEyYkSd73vvdl+fLlWbBgQUaNGpWTTjopSXLLLbfkzjvvzKxZs5L0RPGrX/3qnf0ROeMIAACwyZvf/OY8/vjjWbt2be/YK17xirzxjW/Mpz/96Zx88skveszNN9+cSy65JMuWLcvuu+/eOz5p0qQkyete97occ8wxufvuu1/02BtuuCGLFi3KXXfdlVmzZmX9+vU7POexY8dm1KhRSXr+TuMZZ5zR+9nKe++9NxdddNEOH/OFhCMAAEDnZz/7WTZs2JDx48dvMX7++efn0ksvzb777rvF+N13351zzz03y5Yt2+LM3pNPPpnnn38+Sc9KrLfffnumTZu2xWM3btyYhx56KHPnzs2ll16ap556Ks8880ze+ta35vLLL9/iWLNnz853v/vdPP7449mwYUOuueaa/P7v//6L5j9v3rx84xvfyGOPPZYkWbdu3RYrvb5ULlUFAABGtE2fcUx6ztgtWbKk9wzeJgcffPAWq6lu8md/9md55pln8u53vztJ8prXvCbLli3L6tWrc+6552a33XbLxo0bc+GFF74oHDds2JD3v//9eeqpp1JrzYc+9KHss88++djHPpZFixblkEMOyahRo7J48eK8613vyqc+9anMnTs3tda8/e1vz/z58180n2nTpuWTn/xkjjvuuGzcuDFjxozJ5Zdfnte+9rU79TMqtdadOsBAmjlzZt20ehEAADD8rF69OgcddNBQT2PE2drPvZRyZ6115tb2d6kqAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAMCINXfu3Nx4441bjH3uc5/LH/7hH2bjxo350Ic+lEMOOSSHHnpoZs2alV/84hdJkilTpuSoo47a4nEzZszIIYccMmhzH0yjh3oCAAAAm5SLS78ery5u/936U089NUuXLs3xxx/fO7Z06dL89V//da699to88sgjWbVqVXbbbbesWbMmr3zlK3v3+/Wvf52HHnooBxxwQFavXt2v825Zv359Ro8e3JRzxhEAABixTj755Nxwww35zW9+kyR54IEH8sgjj+Soo47Ko48+mv333z+77daTTZMnT864ceN6H7tw4cJce+21SZJrrrkmp5566laf49FHH83RRx/de0bye9/7XpLk29/+do444ogcdthhmTdvXpJk3bp1WbBgQaZPn545c+Zk1apVSZKLLroop512Wt7ylrfktNNOy9q1a3PSSSdl1qxZmTVrVm6//faB+QF1nHEcofr6Pznb+x8aAAB4Odt3330ze/bs/PM//3Pmz5+fpUuXZuHChSmlZOHChTnyyCPzve99L/Pmzcv73//+HH744b2PPemkk3LmmWfmT//0T/OP//iPufrqq/PlL3/5Rc/x1a9+Nccff3z+4i/+Ihs2bMizzz6btWvX5uyzz87y5cszderUrFu3LkmyePHiHH744bn++utz66235vTTT8/KlSuTJPfcc0++//3vZ4899sh73/vefPjDH86RRx6Z//iP/8jxxx8/oGc9hSMAADCibbpcdVM4XnnllUl6zjDee++9ufXWW3Prrbdm3rx5+frXv957dnD8+PEZN25cli5dmoMOOih77rnnVo8/a9asnHXWWfntb3+bBQsWZMaMGbntttty9NFHZ+rUqUl6AjZJvv/97+e6665Lkhx77LF54okn8vTTTydJ3vGOd2SPPfZIktx888255557ep/j6aefzjPPPJO99tprAH5CwhEAABjh5s+fnw9/+MO566678uyzz+aNb3xj73277757TjzxxJx44omZOHFirr/++t5wTJL3vOc9WbRoUb74xS9u8/hHH310li9fnhtuuCF/8Ad/kD/5kz/Z4pLXvtr885UbN27MD3/4w4wdO3aHj/NS+IwjAAAwou21116ZO3duzjrrrC0+p3jXXXflkUceSdITaqtWrcprX/vaLR77zne+Mx/5yEe2WFznhR588MFMnDgxZ599dj74wQ/mrrvuypw5c7J8+fLeVVo3Xap61FFH5eqrr06S3Hbbbdlvv/3yO7/zOy865nHHHZe//du/7b296XLWgeKMIwAAMOKdeuqpeec735mlS5f2jj322GM5++yz8/zzzydJZs+enfPOO2+Lx+2999654IILmse+7bbb8jd/8zcZM2ZM9tprr3zpS1/KhAkTcsUVV+Rd73pXNm7cmFe/+tW56aabctFFF+Wss87K9OnTs+eee2bJkiVbPeZll12WRYsWZfr06Vm/fn2OPvrofOELX9jJn8K2lVp33cVPZs6cWVesWDHU0xiWLI4DAMCuYPXq1TnooIOGehojztZ+7qWUO2utM7e2v0tVAQAAaBKOAAAANAlHAAAAmoQjAAAwpHbldVeGo5fy8xaOAADAkBk7dmyeeOIJ8ThIaq154okndvjvP/pzHAAAwJCZPHly1qxZk7Vr1w71VEaMsWPHZvLkyTv0GOEIAAAMmTFjxmTq1KlDPQ22w6WqAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA09SkcSykPlFJ+XEpZWUpZ0Y3tW0q5qZTy8+77uG68lFIuK6XcV0pZVUo5YrPjnNHt//NSyhkD85IAAADoTztyxnFurXVGrXVmd/vCJLfUWg9Mckt3O0lOTHJg93VOks8nPaGZZHGSNyWZnWTxptgEAABg17Uzl6rOT7Kk216SZMFm41+qPX6YZJ9Syv5Jjk9yU611Xa31ySQ3JTlhJ54fAACAQdDXcKxJ/qWUcmcp5ZxubGKt9dFu+5dJJnbbk5I8tNlj13Rj2xoHAABgFza6j/sdWWt9uJTy6iQ3lVJ+tvmdtdZaSqn9MaEuTM9Jkte85jX9cUgAAAB2Qp/OONZaH+6+P5bkH9LzGcVfdZegpvv+WLf7w0kO2Ozhk7uxbY2/8LmuqLXOrLXOnDBhwo69GgAAAPrddsOxlPLKUsrem7aTHJfkJ0mWJdm0MuoZSb7ZbS9Lcnq3uuqcJE91l7TemOS4Usq4blGc47oxAAAAdmF9uVR1YpJ/KKVs2v+rtdZvl1LuSPK1UsoHkjyYZGG3/7eSvC3JfUmeTXJmktRa15VSPpHkjm6/v6y1ruu3VwIAAMCA2G441lrvT3LYVsafSDJvK+M1yaJtHOuqJFft+DQBAAAYKjvz5zgAAAAYAfq6qioAI1i5uGx3n7q4XxbXBgB2Qc44AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgafRQTwCAHuXi0qf96uI6wDMBANiSM44AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0DR6qCcAAC9Vubj0ab+6uA7wTABgeHPGEQAAgCbhCAAAQJNwBAAAoEk4AgAA0GRxHAAYQfqyoJDFhAB4IWccAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNo4d6AsDwVS4u292nLq6DMBMAAHaGM44AAAA0CUcAAACahCMAAABNfQ7HUsqoUsrdpZR/6m5PLaX8WynlvlLKtaWUV3Tju3e37+vun7LZMT7ajd9bSjm+v18MAAAA/W9Hzjj+UZLVm92+NMlna62/l+TJJB/oxj+Q5Mlu/LPdfimlTEtySpKDk5yQ5O9KKaN2bvoAAAAMtD6FYyllcpK3J/m/3e2S5Ngk3+h2WZJkQbc9v7ud7v553f7zkyyttT5fa/1FkvuSzO6PFwEAAMDA6esZx88l+UiSjd3t8Un+s9a6vru9JsmkbntSkoeSpLv/qW7/3vGtPAYAAIBd1HbDsZTyP5M8Vmu9cxDmk1LKOaWUFaWUFWvXrh2MpwQAAKChL2cc35LkHaWUB5IsTc8lqv8ryT6llNHdPpOTPNxtP5zkgCTp7n9Vkic2H9/KY3rVWq+otc6stc6cMGHCDr8gAAAA+td2w7HW+tFa6+Ra65T0LG5za631fUm+k+Tkbrczknyz217W3U53/6211tqNn9Ktujo1yYFJ/r3fXgkAAAADYvT2d9mmC5IsLaV8MsndSa7sxq9M8uVSyn1J1qUnNlNr/Wkp5WtJ7kmyPsmiWuuGnXh+AAAABsEOhWOt9bYkt3Xb92crq6LWWp9L8u5tPP6SJJfs6CQBAAAYOjvydxwBAAAYgYQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0jR7qCQAAALzclYtLn/ari+sAz2RgOOMIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAICm0UM9AQCAXdVw/4PeAH3ljCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNo4d6AgAw3JSLS5/2q4vrAM8EAPqHM44AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTVVXhZc7qjQAADDRnHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoGj3UE4CXi3Jx6dN+dXEd4JkAAMDgEo4AAMCL9OU/zf2H+cjhUlUAAACahCMAAABNLlUFRhyfVwUA2DHOOAIAANAkHAEAAGgSjgAAADQJRwAAAJosjgMA8DLkb+wBg8kZRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJPFcQCAIdeXhV4Si70ADBVnHAEAAGgSjgAAADQJRwAAAJqEIwAAAE0WxwEAYNizABPsHGccAQAAaBKOAAAANAlHAAAAmoQjAAAATRbHAQCAIdSXhXss2sNQc8YRAACAJuEIAABAk3AEAACgabufcSyljE2yPMnu3f7fqLUuLqVMTbI0yfgkdyY5rdb6m1LK7km+lOSNSZ5I8p5a6wPdsT6a5ANJNiT5UK31xv5/SbsWf2wWAAB4uevLGcfnkxxbaz0syYwkJ5RS5iS5NMlna62/l+TJ9ARhuu9PduOf7fZLKWVaklOSHJzkhCR/V0oZ1Z8vBgAAgP633TOOtdaa5Jnu5pjuqyY5Nsl7u/ElSS5K8vkk87vtJPlGkv9dSind+NJa6/NJflFKuS/J7CQ/6I8XwvBhZTEAANi19OkzjqWUUaWUlUkeS3JTkv+X5D9rreu7XdYkmdRtT0ryUJJ09z+VnstZe8e38hgAAAB2UX0Kx1rrhlrrjCST03OW8A0DNaFSyjmllBWllBVr164dqKcBAACgj3ZoVdVa638m+U6SNyfZp5Sy6VLXyUke7rYfTnJAknT3vyo9i+T0jm/lMZs/xxW11pm11pkTJkzYkekBAAAwALYbjqWUCaWUfbrtPZK8Ncnq9ATkyd1uZyT5Zre9rLud7v5bu89JLktySill925F1gOT/Ht/vRAAhpFS+vYFAAyK7S6Ok2T/JEu6FVB3S/K1Wus/lVLuSbK0lPLJJHcnubLb/8okX+4Wv1mXnpVUU2v9aSnla0nuSbI+yaJa64b+fTkAAAD0t76sqroqyeFbGb8/PZ93fOH4c0nevY1jXZLkkh2fJgAAAENlhz7jCAAAwMgjHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABA0+ihngAAAMNHubhsd5+6uA7CTID+5IwjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcByOStn+FwAAQB8JRwAAAJqEIwAAAE3CEQCGg758TMFHFQB4iYQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AjuulL59AQAwLAhHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEGMksdAQA9IFwBABGJv9pAtBnwhEAAIAm4QgAAECTcAQYDC6JAwBexoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAANBSyva/hjnhCAAAQJNwhMT/IgEAQINwBAAAoEk4AgAA0CQcAQAAaBKODJ6+fI7QZwkBAGCXIxyB4cN/TgAADAjhCLsyEQQAwC5AOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgabvhWEo5oJTynVLKPaWUn5ZS/qgb37eUclMp5efd93HdeCmlXFZKua+UsqqUcsRmxzqj2//npZQzBu5lAQC8TJXSty+AQdSXM47rk5xfa52WZE6SRaWUaUkuTHJLrfXAJLd0t5PkxCQHdl/nJPl80hOaSRYneVOS2UkWb4pNAHjZ8A96AEag7YZjrfXRWutd3favk6xOMinJ/CRLut2WJFnQbc9P8qXa44dJ9iml7J/k+CQ31VrX1VqfTHJTkhP69dUAAADQ73boM46llClJDk/yb0km1lof7e76ZZKJ3fakJA9t9rA13di2xgEA2NW5hBZGtD6HYyllryTXJfnjWuvTm99Xa61Jan9MqJRyTillRSllxdq1a/vjkAAADGeCFgZcn8KxlDImPdF4da3177vhX3WXoKb7/lg3/nCSAzZ7+ORubFvjW6i1XlFrnVlrnTlhwoQdeS0AAAAMgL6sqlqSXJlkda31M5vdtSzJppVRz0jyzc3GT+9WV52T5KnuktYbkxxXShnXLYpzXDcGAADALmx0H/Z5S5LTkvy4lLKyG/vzJJ9K8rVSygeSPJhkYXfft5K8Lcl9SZ5NcmaS1FrXlVI+keSObr+/rLWu65dXAQAAwIDZbjjWWr+fZFsXhs/byv41yaJtHOuqJFftyAQBAAAYWju0qioAwA6xEifAsCAcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAgJGklL59wWaEIwAAAE3CEQAAgCbhuDOc4gcAgP7j39e7LOEIAAADwWcJGUaEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAptFDPQEAAIC+KheXPu1XF9cBnsnI4owjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3bDcdSylWllMdKKT/ZbGzfUspNpZSfd9/HdeOllHJZKeW+UsqqUsoRmz3mjG7/n5dSzhiYlwMAAEB/68sZxy8mOeEFYxcmuaXWemCSW7rbSXJikgO7r3OSfD7pCc0ki5O8KcnsJIs3xSYAAAC7tu2GY611eZJ1Lxien2RJt70kyYLNxr9Ue/wwyT6llP2THJ/kplrrulrrk0luyotjFAAAgF3QS/2M48Ra66Pd9i+TTOy2JyV5aLP91nRj2xp/kVLKOaWUFaWUFWvXrn2J0wMAAKC/7PTiOLXWmqT2w1w2He+KWuvMWuvMCRMm9NdhAQAAeIleajj+qrsENd33x7rxh5McsNl+k7uxbY0DAACwi3up4bgsyaaVUc9I8s3Nxk/vVledk+Sp7pLWG5McV0oZ1y2Kc1w3BgAAwC5u9PZ2KKVck+SYJPuVUtakZ3XUTyX5WinlA0keTLKw2/1bSd6W5L4kzyY5M0lqretKKZ9Icke331/WWl+44A4AAAC7oO2GY6311G3cNW8r+9Yki7ZxnKuSXLVDswMAAGDI7fTiOAAAAAxvwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgIIozpUAAATrSURBVCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0CUcAAACahCMAAABNwhEAAIAm4QgAAECTcAQAAKBJOAIAANAkHAEAAGgSjgAAADQJRwAAAJqEIwAAAE3CEQAAgCbhCAAAQJNwBAAAoEk4AgAA0CQcAQAAaBKOAAAANAlHAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0DXo4llJOKKXcW0q5r5Ry4WA/PwAAADtmUMOxlDIqyeVJTkwyLcmppZRpgzkHAAAAdsxgn3GcneS+Wuv9tdbfJFmaZP4gzwEAAIAdMNjhOCnJQ5vdXtONAQAAsIsqtdbBe7JSTk5yQq31g93t05K8qdZ63mb7nJPknO7m/0hy76BNcOftl+TxoZ4EDBHvf0Yy739GKu99RrLh+P5/ba11wtbuGD3IE3k4yQGb3Z7cjfWqtV6R5IrBnFR/KaWsqLXOHOp5wFDw/mck8/5npPLeZyQbae//wb5U9Y4kB5ZSppZSXpHklCTLBnkOAAAA7IBBPeNYa11fSjkvyY1JRiW5qtb608GcAwAAADtmsC9VTa31W0m+NdjPO0helpfYQj/x/mck8/5npPLeZyQbUe//QV0cBwAAgJefwf6MIwAAAC8zwrGflFJOKKXcW0q5r5Ry4VDPBwZTKeWBUsqPSykrSykrhno+MFBKKVeVUh4rpfxks7F9Syk3lVJ+3n0fN5RzhIGyjff/RaWUh7vf/ytLKW8byjnCQCilHFBK+U4p5Z5Syk9LKX/UjY+o3//CsR+UUkYluTzJiUmmJTm1lDJtaGcFg25urXXGSFqWmhHpi0lOeMHYhUluqbUemOSW7jYMR1/Mi9//SfLZ7vf/jG4tCxhu1ic5v9Y6LcmcJIu6f+uPqN//wrF/zE5yX631/lrrb5IsTTJ/iOcEQD+rtS5Psu4Fw/OTLOm2lyRZMKiTgkGyjfc/DHu11kdrrXd1279OsjrJpIyw3//CsX9MSvLQZrfXdGMwUtQk/1JKubOUcs5QTwYG2cRa66Pd9i+TTBzKycAQOK+Usqq7lHVYX6oHpZQpSQ5P8m8ZYb//hSPQH46stR6Rnsu1F5VSjh7qCcFQqD1LlVuunJHk80len2RGkkeTfHpopwMDp5SyV5LrkvxxrfXpze8bCb//hWP/eDjJAZvdntyNwYhQa324+/5Ykn9Iz+XbMFL8qpSyf5J03x8b4vnAoKm1/qrWuqHWujHJ/4nf/wxTpZQx6YnGq2utf98Nj6jf/8Kxf9yR5MBSytRSyiuSnJJk2RDPCQZFKeWVpZS9N20nOS7JT9qPgmFlWZIzuu0zknxzCOcCg2rTP5o774zf/wxDpZSS5Mokq2utn9nsrhH1+7/0nFVlZ3XLT38uyagkV9VaLxniKcGgKKW8Lj1nGZNkdJKvev8zXJVSrklyTJL9kvwqyeIk1yf5WpLXJHkwycJaqwVEGHa28f4/Jj2XqdYkDyQ5d7PPfMGwUEo5Msn3kvw4ycZu+M/T8znHEfP7XzgCAADQ5FJVAAAAmoQjAAAATcIRAACAJuEIAABAk3AEAACgSTgCAADQJBwBAABoEo4AAAA0/X9rEXZzs4C9EgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}